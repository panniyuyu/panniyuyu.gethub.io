title: 震惊!系统崩了竟然因为这个
author: YyWang
tags: Java
categories: Java
date: 2020-05-28 17:16:07
---

菜鸟遇到线上问题，从第一次发现问题到问题解决用时2个月，也算是个记录吧，哈哈，本文记录下我解决问题的心路历程，也不是什么高级的问题，大神请绕路

### 第一次问题 
#### 背景
4月27日10:40-11:40一个尾数164的容器CPU突然飙升疯狂收到报警的邮件和消息

#### 应对措施	
* 查看后台日志有一个url在刷屏，条件反射的找到这个刷屏的人给他限流（因为之前有过类似的情况，用户通过程序发送url请求来获取数据，30秒之内精准限流，我也是老手了）
* 继续观察监控，调用量下来了，CPU迟迟不降，没有办法重启了容器

![upload successful](/images/dump问题1.png)

#### 分析
*  CPU不降一定是还在做计算工作
*  排查被刷的url，是一个监控报表的url（由于系统只提供最大1小时的数据，临近618很多用户需要几天甚至几周的数据做分析，不得已通过程序来跑出自己所需的数据）
*  代码走查，是通过调用es的服务，获取数据，再封装成前端需要的数据返回，逻辑上是没问题的
	* 获取数据：查的是es响应很快，2s之内就有数据
	* 前端页面一共3个图，不论查询多少时间间隔的数据都只有90个点，3个图共270个点，也就是后台封装270个对象
* 整个过程也就封装对象是计算的过程需要CPU，难不成这个人用多个线程来跑这个url，这也太变态了吧
* 验证：7个任务，每个任务循环访问500次，查看监控CPU还不到5%，平常2%左右

#### 处理
* 可以排除是获取监控数据这个URL的问题，当天回看日志没有发现其他异常，问了前辈之前也没出现过类似问题，而且手上还有需求就先搁置了；过了一周再排查的时候发现找不到历史日志了，因为刚刚迁移了环境历史日志没有打开，死无对证了。。。
* 也算排查了一个系统的风险点吧，最终，开启系统的历史日志，重新配置报警的阈值，等下次再出问题时摘除负载均衡，保留现场再做排查

### 第二次出现问题
#### 背景
5月26日19:15前后，同样还是收到尾数164的容器CPU飙升的告警邮件，本以为上次的问题是个灵异事件，没想到又出现了，有了上次的经验，这次就很从容，将负载均衡摘除安心排查问题，可是一会又收到了同机房尾数是182的容器的报警邮件，群里的用户也开始反馈系统无法访问了，我愣住了，脑子里都是在想最近有做什么上线吗，是哪里出了问题；在我发愣的时候，前辈已经开始扩容，紧急扩容了4个容器这才稳住局势，这我才回过神来，我还是年轻😂

![upload successful](/images/dump问题2.png)

#### 分析
* 经过上面的处理，下面就开始分析问题了，首先还是看日志没有发现异常，于是和运维同学要了dump文件，运维同学说dump不用看了，容器的jvm内存只有1G？？？想到当时迁移系统时没改配置，使用的是默认的配置，修改了内存配置后解决问题，完事儿！那我写这篇文章也太水了吧，哈哈
* 我要了dump文件，还是要看一下问题的，而且每次都是尾数164的容器出错，太诡异了
* 使用JProFiler分析dump，可以看到百万数量级的引用4类，结合最大的对象来看，是一个超大的list，占用了80%的内存

![upload successful](/images/dump问题3.png)
![upload successful](/images/dump问题4.png)

* 代码走查，list中的对象是事件的对象，用来同步数据用，类似消息队列，再结合日志来看，马上定位到了一个用户再操作实例的上线（更改实例中所有接口的状态，产生的事件是为了同步数据），并产生了300w+的事件，系统封装事件后终于把内存撑爆，进行FullGC，Stop The Word
* 查看日志和用户操作的记录，我寻思对一个实例操作怎么会有这么多事件，不会是有死循环了吧；先排查了不是多线程的问题

```
for (Server server : servers) {
    // 外层循环生产事件
    eventList.add(buildEvent(server));
    for (IfaceAlias ifaceAlias : ifaceAliasList) {// 内层循环与外层循环的数据一样
        if (condition) {
            eventList.add(buildEvent(ifaceAlias));
        }
    }
}
// 伪代码，只是为了展示代码结构
```

* 上线的逻辑是这样
	* a.数据库中修改实例的状态（两个sql用时3s内）
	* b.封装要操作的事件
	* c.发送事件 （调用远程服务，远程服务没有接受到事件的日志，定位在b中出现问题）
* 步骤b中，嵌套了两层循环来生产事件，逻辑如下（以前代码的逻辑，为什么这么做就不知道了，前辈的东西先不动），我查了线上的数据，这个实例中有282个接口，两层循环也就8w不到的事件啊
  * 验证：将线上数据拷贝下来，本地测试，产生6000+的事件也不到8w啊？debug后才知道，两层嵌套循环符合条件的只有77个（业务问题不需要关心），77*77+外层循环生产的事件刚好6000+数量级，这和线上的百万数量级差好多哦，这多出来的是从哪来的呢
  * 想了两天没有结果，忍不住了，我找到用户的联系方式直接问，原来操作了20多个实例，发现用户操作的日志超长了，后面的内容截断了，让我以为就操作了一个实例
  * 验证：按照25个实例来算，每个实例两层循环需要重新计算外层循环x内存循环（25x77）x（25x77）结果证实300w+的数量级，破案

### 总结

主要问题还是迁移系统之后jvm设置的内存有问题，连带找出其他bug（以外收获）

1. 封装事件代码有问题，两层嵌套循环产生很多重复事件（不是说之前写代码的人太水，而是业务复杂，刚好用户的数据结构特殊导致产生大量重复事件）
2. 用户操作的日志记录不完整，误导了排查错误的方向，这是个风险
3. jvm内存设置的有问题，即使上面两个问题不修改，源码8g内存都用上的话运行5年的话也不会出问题
4. 直接查看jvm的配置或者找用户了解情况就不会走这么多弯路，这条里就是经验了，本文的精华都在这一句了


	
