title: 一年菜鸡带你看微服务
author: YyWang
tags: Java
categories: Java
date: 2020-06-25 16:36:16
---

工作快一年了，我从一个刚入职时候的小白变成了现在有一年经验的小白,在我这一年的工作中修炼自己，现在可以说是把自己的一条腿抬过了微服务的门槛，在这个位置看到了一下东西，把我的理解记录分享一下。

## 前言
随着历史进程的发展，我们的业务量也愈发膨胀，系统的也从原来的单体架构，逐渐演化成微服务架构，将重复的业务逻辑抽象成基础的服务，这样做不仅仅可以将业务解耦，还通过将一个大的系统拆分成服务粒度的子系统，各个子系统单独开发编译测试上线，大大提高了开发的效率，加快了系统迭代的速度。根据我的理解，微服务框架包括RPC框架和注册中心两个部分，下面就来分别介绍一下

### 注册中心
微服务框架是用来进行远程调用，涉及到远程调用就会有服务的提供方（provider）和调用方（consumer），注册中心的工作就是让他们彼此认识，即服务注册和服务发现，除此之外注册中心还负责将对方状态的变化通知到另一方，配置下发等功能，主要有CP和AP两种选型的注册中心

* CP 强一致性 Zookeeper、etcd ...
	* 用Zookeeper举例来说，发布服务就会创建一个目录，在该目录下维护服务提供方和调用方的信息，通过zk本身的watch机制来观察服务状态变化
	* 缺点是当服务规模很大时，zk需要数据同步的时间变长，期间服务不可用，也就是选择CP会出现的问题；数据同步的请求量大很可能将leader打挂（比如说我了解到曾经有服务大规模扩容，为了保证顺序所有follower将写请求发送给leader，大量写请求导致leader宕机，重新选择leader期间服务不可用，新的leader瞬间又被打挂）
* AP 弱一致性 消息总线型
	* 服务注册和订阅的数据全量保存在一个存储介质中，注册中心从存储介质中获取的数据缓存在内存中，通过消息总线做数据同步（推拉结合，用版本号保证消息的顺序性）
	* 服务调用方从注册中心获取数据保存在调用方的内存中，同样采用推拉结合，保证最终一致性；
		* 推 callback机制，服务调用方在生成代理的目标对象之前会向注册中心订阅配置，订阅的时候会将callback序列化发送给注册中心，注册中心收到带有callback参数的请求为callback创建代理对象，相当于是一个反向的RPC调用（服务提供方向服务调用方发送请求复用已有的tcp连接）
		* 拉 心跳，服务调用方通常会向注册中心发送心跳来告知自己的状态，发心跳的同时拉取需要更新的数据到内存中

### RPC框架
RPC框架用于服务之间的通信，例如Dubbo、SpringCloud、gRPC，通过不同的协议完成服务之间的调用；除了最基本的通信以外还包括服务治理策和流量控制的策略，服务监控和追踪，等其他特性

#### 服务治理&流量控制

* 服务管理
	* 服务节点的新增删除；分组，动态分组的设置与取消；不同维度的上下线
* 路由策略
	* 调用者将请求路由至固定的一个或者多个服务提供方的节点；用于测试，或者灰度发布
* 负载均衡
	* 将服务调用方的请求均匀的发送给选择到的服务提供方节点；随机、加权随机、轮询、加权轮询、最少活跃、一致性哈希，之前分析过不同的策略实现，移步[这里](http://yywang.top/2020/03/27/%E6%8F%AD%E7%A7%98%E4%BA%AC%E4%B8%9C%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AE%97%E6%B3%95/)查看
* 容错策略(集群策略)
	* failover 失败自动切换不同的服务提供方进行重试 （业务错误重试和安全重试？）
	* failfast 快速失败，失败
	* failsafe 失败安全，出现异常直接忽略，用于记录审计日志
	* broadcast 广播调用，失败的跳过
	* pinpoint 点对点调用
* 限流 用于保护服务提供方（按APP，接口，分组，IP，方法不同的维度）在服务提供方和调用方都可以实现限流的逻辑
	* 在服务提供方的限流逻辑只是针对于单个容器或者物理机，服务提供方只需处理小于最大限流值的请求即可，不需要关心其他的服务节点，几乎没有性能的损耗，具体的限流算法之前写过一篇限流算法的[介绍](http://yywang.top/2020/05/14/get%E9%99%90%E6%B5%81%E5%A7%BF%E5%8A%BF/#more)
	* 在服务调用方做限流逻辑可针对整个集群的维度，一般是先请求集中式的服务来判断本次请求是否超过限流值，多了一次rpc请求，会影响性能，在京东微服务框架中，限流逻辑在服务调用方有monitor和counter两种实现方式；monitor服务，对es中的监控日志进行统计异步判断是否超过限流值，因为是异步请求不影响性能，但限流的实时性无法保证不完全精准，目前这种方式已经废弃；counter服务，通过访问缓存数据库实时统计请求，判断是否超过限流值，精度准确，会影响性能；
* 熔断，用于保护服务调用方
	* 防止服务调用方依赖下游服务异常，挂起大量请求压垮容器，比如服务调用方依赖下游的服务，由于下游的服务异常导致短时间内无法收到响应而一直挂起，如果挂起的请求过大可能会使服务不可用，熔断策略可以快速返回失败从而保护服务不被打挂掉
	* 熔断器（打开，半打开，关闭三种状态；服务调用的失败率高于某一阈值，新的请求直接返回失败不进行处理，进入半打开状态；半打开状态一段时间后，开始处理新的请求，如果失败率仍然高于阈值，则进入关闭状态否则进入打开状态）
* 分组
	* 业务流量隔离，将核心业务与其他业务隔离开来
	* 控制同机房调用，不同的机房设置不同的分组，通过分组来实现相同机房的服务以及互相调用
	* 动态分组；可以用来流量切换；可以应对突发的流量激增，分组中预留的容器依然不能顶住流量，可以借用其他分组中容器来分担一部分流量
* 流量回放；服务调用方异步存储请求和响应，用来测试

#### 服务监控和追踪
用来查看请求量（TPS，实时和历史数据）、响应时间

* 服务监控 数据采集方式 a)通过服务主动上报，将日志数据发送到日志节点 b)通过代理收集，将日志数据保存到本地，代理解析本地日志（sidecar）
* 服务追踪 整条链路用TraceId标记，其中经过的每个服务节点都有不同的SpanId，最终通过对数据结构的解析绘制链路图

#### 其他特性
* 健康检查，通过发送心跳报告节点的状态
  * 比如在京东微服务框架中调用方隔30s向内存中的服务提供方列表发送心跳（探活），探活不同结果将列表分成不同的状态（健康、亚健康、死亡），调用方每次从健康节点的集合中选择一个发起调用；亚健康状态只发心跳不发请求，当心跳恢复移至健康节点，健康->亚健康（连续6次心跳失败，异常心跳可重试两次，三次都失败则当前心跳失败），连续60次心跳失败加入重连的集合中，10s重连一次
  * 服务提供方和服务调用方都会想注册中心发送心跳报告状态，30s，服务提供方连续8小时没有向注册中心发送心跳会删除
* 优雅启停
	* 停止；拒绝新的请求，返回一个正在关闭的异常，调用方安全重试->在超时时间范围内处理完已经接受的请求，时间范围以外的返回异常->开始关闭动作释放资源
	* 启动；启动预热，随着启动时间的增加逐步增加流量，定时动态修改负载均衡策略的权重；提前加载缓存数据；注册之前模拟调用逻辑保证服务可用->延迟暴露，待所有的服务完全启动完再想注册中心注册服务
* 泛化调用；在没有接口和API的情况下发起调用，将服务提供方的接口名，分组，方法名，参数发送给服务提供方，随后根据信息通过反射调用本地方法（1.测试平台，无需修改配置重启再发起调用；2.统一网关，无需引入jar包）
* 安全问题；服务调用方通过引入jar包的方式来发起调用，可能会发生在服务提供方不知情的情况下发起调用，成为压垮服务的最后一根稻草
	* 服务的提供方和调用方商量一个token，服务提供方收到请求在filter中校验隐式传参的的token判断是否通行
	* 在京东微服务框架中还可以再服务治理平台中开启APP调用，只有申请了APP调用被服务提供方审核之后才能从注册中心中获取服务列表

#### 实现原理
将远程调用的过程通过动态代理封装起来，与业务逻辑解耦，使用者可以专心于业务逻辑而不必关心RPC调用的过程；

* 服务提供方启动
	* 与注册中心建立连接，订阅全局配置（远程callback机制，发送请求将callback序列化发送出去，接收方对callback进行动态代理回调发送方）
	* 开启并暴露服务的端口，向注册中心注册
* 服务调用方启动
	* 与注册中心建立连接，订阅全局配置
	* 创建动态代理对象，构造执行链 filter chain（内置的filter->自定义的filter->最后的filter根据配置生成路由，负载均衡策略的逻辑）注入到代理对象中
	* 向注册中心订阅服务列表
* 调用方发起调用
	* 调用前首先运行执行链，最后的filter过滤出服务提供方节点（本地调用->路由策略->黑白名单->负载均衡）
	* 序列化请求的参数，封装协议报文，根据配置的集群容错策略通过netty框架向服务提供方发送请求，
* 服务方处理请求
	* 接收到请求首先适配协议，根据协议和序列化、压缩的方式解析请求
	* 运行执行链，最后的filter会根据接口名、方法和参数信息通过反射调用本地方法，最后将接口返回给调用方，调用方进行反序列化得到响应结果

## Service Mesh 服务网格
微服务发展到今天一些弊端也逐渐显露出来，最大的痛点就是业务入侵严重，使用者通过引入jar包使用微服务框架，如果要增加新的功能点或者修复一个紧急的bug就需要告诉所有使用方升级一下jar包，这会非常困难，可能你会说maven修改一下版本不就好了，但是在京东如果非常重要的业务比如交易服务，是不希望去做改变的，而且服务上下有着非常复杂的依赖关系牵一发而动全身；另外微服务框架不支持跨语言，对应微服务框架的不同语言直接想要互相调用得需要多个语言开发的框架版本才行，这对于开发和维护的成本都非常大。

Service Mesh完美的解决了这两个痛点，可以简单将其理解为将RPC框架的内容抽象成sidecar，微服务框架的jar包中只保留与sidecar通信的总要逻辑即可，这些逻辑基本上是不会变的，当发布你的应用容器的时候注入sidecar，原来是容器中的应用直接互相调用的方式现在变成了容器中的应用与容器中的sidecar通信，sidecar通过iptables规则做请求转发，并且服务治理，流量控制的这些逻辑抽象到了sidecar中这对于业务方来讲只需重新注入sidecar即可完成升级，对于应用来讲升级过程是透明的。

![upload successful](/images/微服务.png)

