<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一张图看懂CPU]]></title>
    <url>%2F2020%2F03%2F20%2F%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%9C%8B%E6%87%82CPU%2F</url>
    <content type="text"><![CDATA[废话不说，直接上图 CPU可以拆解为三个部分 控制单元 理解为软件设计中的controller，控制数据的流向以及执行计算机指令 数据单元 用做内存的缓存，存放CPU运算的中间结果 运算单元 负责运算，例如加、减、移位这样 简单来说CPU的工作过程是这样的：获取进程中的的指令（程序代码编译运行后的二进制），然后执行这些指令，在此过程中控制单元执行的一些指令，从内存中加载数数据到数据单元，指挥运算单元进行运算，将结果返回数据单元，最后将结果从数据单元写会内存 往细了说 控制单元中有一个指令指针寄存器和一个指令寄存器，控制单元通过下一条指令在内存中的地址，找到该指令并存入指令寄存器；控制单元中还有一个指令起始寄存器和一个数据起始寄存器，这就是进程切换中上下文的概念了，进程1切换到进程2，将进程1的状态分别保存在指令起始地址寄存器（进程1执行到了哪一行代码）和数据起始地址寄存器（进程1数据读到了哪一行）； 再往细了说 先看数据单元，数据寄存器用来保存数据段（内存中分配给进程存放变量的区域，代码段指内存中存放运行代码的区域）的偏移量，数据段的起始地址在控制单元的寄存器中，起始地址+偏移量 就可以得出读出的数据； 就以x86架构的经典处理器8086来说，数据寄存器有8个16位通用寄存器 AX、BX、CX、DX、SP、BP、SI、DI ，其中前4个寄存器分别可以单独拆分为2个8位的寄存器来使用，这就可以获取短的数据或者长的数据，很灵活； 再看控制单元，CS和DS分别保存代码段的起始地址（指令的起始地址）和数据段的起始地址，C为code，D为data；SS是栈寄存器，存放函数的调用关系；IP寄存器存放下一条指令的地址 8086的寄存器都是16位的，可地址总线是20位的，所以说在内存中寻址的时候起始地址需要左移4位再加偏移量得到最终的地址 再再往细了说 32位处理器的数据单元将原来16位的寄存器扩展到32位，并且为了兼容保持了原始16位寄存器的结构；控制单元中的段寄存器（CS,DS,SS,ES）变化较大，其中CS,DS,SS,ES还是16位，只是不在存起始地址，真正的地址在后面16位中的段描述符缓存器里，而端寄存器中存的东西叫做选择子，来选择段描述符缓存器中的地址信息 再再再往细了说呢 没有了，哈哈，我就学习到这里了]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将Hexo博客迁移到docker（二）]]></title>
    <url>%2F2020%2F02%2F14%2F%E5%B0%86Hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0docker%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本篇将进行迁移的第二阶段，主要步骤为 在git上备份博客中的文件 进入docker容器中还原 验证 修改Dockerfile 验证 在git上备份博客中的文件hexo d 是将静态文件发布到git上，内容是 public 文件夹中的文件，hexo g 命令会重新生成静态文件；那么其他文件就是我要转移的文件了，将其他文件备份到git仓库中的新分支中 （.gitignore 里给出存放不需要备份的文件，至于为什么后面慢慢了解，本篇重点不在这） 1234567891011121314151617181920212223242526# 新建分支 hexogit clone $&#123;git path&#125;cd username.github.iogit batch hexogit add .git commit -m &apos;初次提交&apos;git push origin hexo# 删除所有文件 后提交分支rm -rf *git add .git commit -m &apos;删除文件&apos;git push origin hexo# 这时候在username.github.io的文件夹下就有了.git文件，将其拷贝的博客目录中mv .git /usr/local/myblogcd /usr/local/mybloggit add .git commit -m &apos;备份博客文件&apos;git push origin hexo# 在 themes/next/ 目录下的部分文件没得了，博客里的相册功能依赖这里的文件，需要处理一下rm -rf /usr/local/myblog/themes/next/.gitignorecd /usr/local/mybloggit add .git commit -m &apos;next主题相关文件&apos;git push origin hexo 到这里已经将自己博客下面的文件都提交到git的hexo分支中了 进入docker容器中还原1234567891011121314151617181920212223242526272829303132333435cd /usr/localgit clone $&#123;git path&#125;cd /usr/local/$&#123;username&#125;.github.iogit checkout hexo# 由于我已经有myblog的文件夹了这离要删除一下rm -rf /usr/local/myblogmv /usr/local/$&#123;username&#125;.github.io /usr/local/myblog# 安装package.json中的依赖# 修改下载源，安装更快npm config set registry https://registry.npm.taobao.orgnpm install hexo --savenpm install hexo-admin --savenpm install hexo-deployer-git --savenpm install hexo-generator-archive --savenpm install hexo-generator-baidu-sitemap --savenpm install hexo-generator-category --savenpm install hexo-generator-feed --savenpm install hexo-generator-index --savenpm install hexo-generator-search --savenpm install hexo-generator-searchdb --savenpm install hexo-generator-sitemap --savenpm install hexo-generator-tag --savenpm install hexo-helper-live2d --savenpm install hexo-renderer-ejs --savenpm install hexo-renderer-marked --savenpm install hexo-renderer-stylus --savenpm install hexo-server --savenpm install hexo-tag-cloud --savenpm install hexo-wordcoun --save# 重新生成静态文件cd /usr/local/mybloghexo cleanhexo ghexo d 验证在浏览器中访问 http://${ip}:8088 效果相同即为成功 修改Dockerfile因为要相册相关要用到python3，镜像中自带的时python2，所以要安装一下python3，在第二阶段的Dockerfile基础上增加下面操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 安装依赖RUN yum update -y &amp;&amp; yum install -y zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make \# 备份原始的python&amp;&amp; mv /usr/bin/python /usr/bin/python.bak \ # 下载解压&amp;&amp; cd /usr/local \&amp;&amp; wget https://www.python.org/ftp/python/3.6.2/Python-3.6.2.tar.xz \&amp;&amp; tar -xvJf Python-3.6.2.tar.xz \# 编译安装&amp;&amp; cd Python-3.6.2 \&amp;&amp; ./configure prefix=/usr/local/python3 \&amp;&amp; make &amp;&amp; make install \&amp;&amp; rm -rf /usr/local/Python-3.6.2.tar.xz \# 添加软链&amp;&amp; ln -s /usr/local/python3/bin/python3 /usr/bin/python3 \&amp;&amp; ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 \# 安装依赖&amp;&amp; pip3 install Pillow \# 迁移博客 由于clone速度极其慢，改用本地先clone好复制过去&amp;&amp; rm -rf /usr/local/myblogCOPY myblog /usr/local/myblog/# &amp;&amp; cd /usr/local \# &amp;&amp; git clone git@github.com:panniyuyu/panniyuyu.github.io.git \# &amp;&amp; cd /usr/local/panniyuyu.github.io \# &amp;&amp; git checkout hexo \# 由于我已经有myblog的文件夹了这离要删除一下# &amp;&amp; rm -rf /usr/local/myblog \# &amp;&amp; mv /usr/local/panniyuyu.github.io /usr/local/myblog \# 安装package.json中的依赖# 修改下载源，安装更快RUN npm config set registry https://registry.npm.taobao.org \&amp;&amp; npm install hexo --save \&amp;&amp; npm install hexo-admin --save \&amp;&amp; npm install hexo-deployer-git --save \&amp;&amp; npm install hexo-generator-archive --save \&amp;&amp; npm install hexo-generator-baidu-sitemap --save \&amp;&amp; npm install hexo-generator-category --save \&amp;&amp; npm install hexo-generator-feed --save \&amp;&amp; npm install hexo-generator-index --save \&amp;&amp; npm install hexo-generator-search --save \&amp;&amp; npm install hexo-generator-searchdb --save \&amp;&amp; npm install hexo-generator-sitemap --save \&amp;&amp; npm install hexo-generator-tag --save \&amp;&amp; npm install hexo-helper-live2d --save \&amp;&amp; npm install hexo-renderer-ejs --save \&amp;&amp; npm install hexo-renderer-marked --save \&amp;&amp; npm install hexo-renderer-stylus --save \&amp;&amp; npm install hexo-server --save \&amp;&amp; npm install hexo-tag-cloud --save \&amp;&amp; npm install hexo-wordcoun --save \# 重新生成静态文件&amp;&amp; cd /usr/local/myblog \&amp;&amp; hexo clean \&amp;&amp; hexo g 验证启动docker容器 绑定端口映射 8088:80 浏览器访问 http://${ip}:8088 查看效果无误，完成 最后总结一下需要迁移的步骤 git push origin hexo推送博客所有文件 编辑Dockerfile 在Dokerfile目录下git clone 博客文件 再切换hexo分支 重命名为myblog 在Dockerfile目录下编辑nginx.conf文件 使用Dockerfile生成镜像 启动容器 绑定端口 进入容器启动nginx 使用Dockerfile生成镜像 启动容器 绑定端口 是不是迁移起来非常简单，可以将生成的镜像备份成tar包，在任意的服务器上安装docker后，还原镜像启动容器即可 相册相关 我的相册是参考这里弄得；我将它移至博客文件的hexo分支，一起备份起来，要上传新的文件运行目录中的tool.py脚本，将照片裁剪后上传至github仓库，这时照片就有了URL，在博客中就可以看到了 发布博客相关 docker容器中的 /usr/local/source/_posts/ 目录下的文件名为乱码，下面方法可以解决，但是我这里没有成功，通过tab补全是正常的ls和ll看就有问题 123yum -y install convmvconvmv -f GBK -t UTF-8 --notest -r /usr/local/source/_posts/ 所有我觉得在宿主机建立文件映射，然后进入docker中hexo g -d更新 hexo d 会失败，这里要重新生成sshkey 12345ssh-keygen -t rsa -C &quot;$&#123;email&#125;&quot;# 拷贝sshkey到github中# 配置git config --global user.name &quot;$&#123;username&#125;&quot;git config --global user.email &quot;$&#123;email&#125;&quot;]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将Hexo博客迁移到docker（一）]]></title>
    <url>%2F2020%2F02%2F13%2F%E5%B0%86Hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0docker%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本篇是迁移工作的第一阶段 docker入门移步到这里 在docker容器中重新搭建Hexo博客系统并记录步骤拉取centos镜像 -&gt; 启动容器 -&gt; 进入容器bash -&gt; 搭建博客 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 拉取镜像docker pull centos:7# 启动容器docker run -di --name=centos7 centos:7# 进入命令行docker exec -it centos7 /bin/bash# 搭建hexo博客# 安装node.js# 安装wgetyum install -y wget# 新建目录 mkdir /usr/local/nodejs# 下载tarwget https://nodejs.org/dist/v12.15.0/node-v12.15.0-linux-x64.tar.xz# 解压xz -d node-v12.15.0-linux-x64.tar.xz# 部署bin文件ln -s /usr/local/nodejs/node_12.15.0/bin/node /usr/local/bin/nodeln -s /usr/local/nodejs/node_12.15.0/bin/npm /usr/local/bin/npm# 安装hexonpm install -g hexo-cli# 安装gityum install git-core# 配置环境变量ln -s /usr/local/nodejs/node_12.15.0/bin/hexo /usr/local/bin/hexo# 创建网站文件夹mkdir myblogcd myblog# 初始化hexohexo inithexo generate# 安装NGINX# 安装依赖yum install -y gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel# 下载NGINXwget https://nginx.org/download/nginx-1.16.1.tar.gz# 解压tar -zxf nginx-1.16.1.tar.gzcd nginx-1.16.1# 编译安装./configure make &amp;&amp; make install# 配置NGINXvim /usr/local/nginx/conf/nginx.conf# 启动NGINXcd /usr/local/nginx/sbin./nginx 其中nginx.conf为 123456789101112131415161718192021# http中server模块做修改即可server &#123; listen 80 default_server; listen [::]:80 default_server; server_name _; root /usr/local/myblog/public/; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / &#123; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; &#125; 完成上述步骤后开始验证 1234567# 将容器保存为镜像docker commit centos7 mycentos:7.1# 启动新的容器docker run -di --name=centos7.1 -p 8088:80 mycentos:7.1 # 进入容器启动NGINX（从镜像启动容器并没有把NGINX启动）docker extc -it centos7.1 /bin/bash# 浏览器中访问 http://$&#123;ip&#125;:8088 验证 编写dockerfile根据上述的步骤一步步编写Dockerfile；然后进行 build -&gt; 报错 -&gt; 进入容器查看错误（我太菜了不能看日志直接修改Dockerfile） -&gt; 修改Dockerfile -&gt; build -&gt; … 直到成功 最终Dockerfile如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546FROM centos:7MAINTAINER yywang sbsbjs@qq.com# 安装依赖RUN yum update -y &amp;&amp; yum install -y wget git-core vim* gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel \# 安装nodejs# 新建目录 WORKDIR /usr/local# 下载tarRUN wget https://nodejs.org/dist/v12.15.0/node-v12.15.0-linux-x64.tar.xz \# 解压&amp;&amp; tar -xvf node-v12.15.0-linux-x64.tar.xz \&amp;&amp; mv node-v12.15.0-linux-x64 node_12.15.0 \&amp;&amp; mkdir /usr/local/nodejs \&amp;&amp; mv node_12.15.0 /usr/local/nodejs/ \&amp;&amp; rm -rf node-v12.15.0-linux-x64.tar.xz \# 部署bin文件&amp;&amp; ln -s /usr/local/nodejs/node_12.15.0/bin/node /usr/local/bin/node \&amp;&amp; ln -s /usr/local/nodejs/node_12.15.0/bin/npm /usr/local/bin/npm \# 安装hexo&amp;&amp; npm install -g hexo-cli \# 配置环境变量&amp;&amp; ln -s /usr/local/nodejs/node_12.15.0/bin/hexo /usr/local/bin/hexo \# 创建网站文件夹&amp;&amp; mkdir /usr/local/myblog \&amp;&amp; cd /usr/local/myblog \# 初始化hexo&amp;&amp; hexo init \&amp;&amp; hexo generat \# 安装NGINX依赖&amp;&amp; cd /usr/local \# 下载NGINX&amp;&amp; wget https://nginx.org/download/nginx-1.16.1.tar.gz \# 解压&amp;&amp; tar -zxf nginx-1.16.1.tar.gz \&amp;&amp; cd /usr/local/nginx-1.16.1 \# 编译安装&amp;&amp; ./configure \&amp;&amp; make &amp;&amp; make install \&amp;&amp; rm -rf /usr/local/nginx-1.16.1 \&amp;&amp; rm -rf /usr/local/nginx-1.16.1.tar.gz 最后~ 验证 1234567# 构建镜像docker build -t mycentos:7.2# 启动容器docker run -di --name=centos7.2 -p 8088:80 mycentos:7.2# 进入容器修改nginx.conf并启动NGINXßdocker extc -it centos7.1 /bin/bash# 浏览器中访问 http://$&#123;ip&#125;:8088 验证]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将Hexo博客迁移到docker中（总）]]></title>
    <url>%2F2020%2F02%2F13%2F%E5%B0%86Hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0docker%EF%BC%88%E6%80%BB%EF%BC%89%2F</url>
    <content type="text"><![CDATA[博客已经运行了大半年了，马上就要到期了，我得着手把搭建的博客备份一下了，续费很贵的话我得找一个便宜的用（我就是一个抠门怪哈哈），最简单的就是做系统的镜像，这也太low了吧。当下流行的是容器技术，决定把先把博客迁移到docker中，如果要换服务器的话直接备份docker就好了呀。 难点：搭建博客的时间过去很久，没有记录，完全忘了如何搭建，不知道都需要迁移哪些文件而且我还搞了很多花里胡哨的东西，这些东西也需要保留 这个就麻烦了，相当于重新搭建Hexo的博客了，没办法谁让我之前没有记录总要还的嘛 迁移步骤分为两个阶段： 第一阶段 重新搭建Hexo博客系统 入门docker 在docker容器中重新搭建Hexo博客系统并记录步骤（启动容器进行端口映射 8088:80，在公网验证） 根据记录的步骤编写Dockerfile，生成镜像（启动容器进行端口映射 8088:80 在公网验证） 第二阶段 迁移博客 总结需要的文件 拷贝文件到容器中 修改Dockerfile 在公网验证 验证成功后，将宿主NGINX端口映射到docker容器中 最终将容器打包成tar做备份，或者将镜像提交到仓库中直接拉取即可，非常完美]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker入门]]></title>
    <url>%2F2020%2F02%2F12%2FDocker%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[开门见山，docker是一种新的虚拟化技术，体积小，启动快，减小了开发和运维成本；下面就简单扫个盲入个门 虚拟技术 传统的虚拟机技术 它的层次结构为： 个人pc（硬件） -&gt; 操作系统（Host OS） -&gt; 虚拟机管理系统（Hypervisor）-&gt; 虚拟机（VM） 虚拟机中的层次为：操作系统（windos/macos…） -&gt; 依赖库（C++…） -&gt; 应用（tomcat/nginx…） docker虚拟技术 它的层次结构为： 个人pc -&gt; 操作系统 -&gt; docker -&gt; 依赖库 -&gt; 容器 容器中的层次接口给为：依赖库（可以复用宿主机的依赖库） -&gt; 应用 总结 docker虚拟技术的层级更少，而且还可以复用宿主机的一些文件（依赖库等），所以docker容器的大小比虚拟机要小很多，并且启动也非常快；大致原理是利用Linux中namespace机制将进程进行隔离，从外部来看就像是运行在容器中一样，docker可以说是进程间的隔离，而虚拟机技术是基于硬件的隔离，在虚拟的硬件基础上又有着不同的操作系统，相比之下docker容器又小又快 比如部署一个nginx；在传统的虚拟机技术下，要先安装一个Linux操作系统的虚拟机，然后在虚拟机上部署nginx；而在docker技术下，无需再安装Linux操作系统，直接复用宿主机的文件和内核即可（若宿主机是Centos，容器是Ubuntu，那就使用Ubuntu的文件，复用宿主机的内核），在此基础上启动nginx进程，并与宿主机的进程隔离，这样nginx容器就部署起来了 除此以外，将自己的应用和环境打包成docker镜像，只要有docker的地方都可以运行相同的容器，不再会有因为环境不同应用运行效果不一样的问题，减小了运维成本，一句话说docker技术解决了应用打包发布的问题 docker一些概念 镜像 用于创建容器的模板 容器 独立运行的一个或一组应用 镜像相当于类，容器相当于类的实例 仓库 用于保存镜像，有公有私有两种，类似于git仓库 常用命令查看版本1docker -v 修改镜像源1vim /etc/docker/daemon.json 启动/停止/重启/查看状态1systemctl start/stop/restart/status docker 查看镜像1docker images 搜索镜像1docker search $&#123;image name&#125; 拉取镜像 不指定版本号拉去最新的1docker search pull $&#123;image name&#125;:$&#123;version&#125; 删除镜像 -f 强制删除1docker rmi -f $&#123;image name/id&#125; 查看正在运行的容器 -a(查看所有)1docker ps -a 容器运行相关参数 -i：表示运行容器 -t：表示容器启动进入命令行 交互式容器 exit退出命令行，容器也退出（守护式容器不会退出） –name：为创建的容器命名 -v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录） -d: 守护模式容器 -p: 表示端口映射 前者宿主机端口 后者容器内映射端口 -e: 指定环境变量 启动交互式容器1docker run -it --name=$&#123;name&#125; $&#123;image name&#125;:$&#123;version&#125; /bin/bash 启动守护式容器1docker run -di --name=$&#123;name&#125; $&#123;image name&#125;:$&#123;version&#125; eg: 1docker run -di --name=mysql_test -p 3316:3306 -e MYSQL_ROOT_PASSWORD=root centosz:7 进入容器1docker exec -it $&#123;container name&#125; /bin/bash eg: 1docker exec -it mysql_test /bin/bash 启动/停止容器1docker start/stop $&#123;container name/id&#125; 宿主机和容器文件互拷宿主-&gt;容器 1docker cp $&#123;file&#125; $&#123;container name&#125;:$&#123;path&#125; 容器-&gt;宿主 要在宿主机中使用命令行 1docker cp $&#123;name&#125;:$&#123;file&#125; $&#123;path&#125; 目录挂载1docker run -di -v $&#123;source path&#125;:$&#123;target path&#125; --name=$&#123;container name&#125; $&#123;image name&#125;:$&#123;version&#125; 查看容器ip1docker inspect $&#123;container name/id&#125; 删除容器1docker rm $&#123;container name/id&#125; 将容器保存为镜像1docker commit $&#123;container name&#125; $&#123;inage name&#125; 将镜像保存为tar包1docker save -o $&#123;tar name&#125;.tar $&#123;path&#125; 恢复镜像1docker load -i $&#123;tar name&#125;.tar 停止全部容器1docker stop $(docker ps -q) 删除全部容器1docker rm $(docker ps -aq) 停止并删除全部容器1docker stop $(docker ps -q) &amp; docker rm $(docker ps -aq) dockerfile 创建镜像简单来说记录一系列命令和参数，然后docker根据dockerfile中的命令来构建镜像 From从哪个基础镜像进行构建 MAINTAINER镜像创建者 ENV key value设置环境变量 RUN command运行shell指令（多个RUN会有多层嵌套，不期望使用多个RUN，多个指令以 \ 结尾 &amp;&amp; 开头） ADD source_file dest_file将宿主文件复制到容器内，压缩文件自动解压 COPY source_file dest_file将宿主文件复制到容器内，压缩文件不自动解压 WORKDIR path设置工作目录，相当于 cd 构建镜像命令1docker build -t $&#123;image name&#125;:$&#123;tag&#125; .]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年终总结（新的开始）]]></title>
    <url>%2F2019%2F12%2F31%2F2019%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%EF%BC%88%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[光阴似箭岁月如梭，时光如水生命如歌，转眼间2019年到了最后的时刻，这一年对我来说是不平凡的，来盘点一下自己这一年都做了什么，主要有两件大事，一个是研究生毕业，一个是入职，上半年忙着毕业，下半年忙着上班。接下来进入回忆 博客 上半年基本一心扑在毕业的事情上，在等发毕业证的时间比较无聊，游戏已经打吐了，随后和马哥（实验室同门）一起租了服务器搭了博客，我选了Next，他选了另一个主题，我们俩就各自开始装修，再后来我弄好了相册功能，奈何他的主题相册的资料很少，所以这个不要脸的就复制了我的主题，博客搭好了之后也更新什么，打算工作后再说 EOS 这个EOS入坑也是马哥带的，记得是18年第二季度忘了是哪个月，比特币大涨在马哥的蛊惑下研究了一番当时EOS很火，是第三代区块链技术吹得特别的牛，没忍住入坑，￥58左右入坑，后面涨到了￥158，心态开始膨胀，当时的舆论也是一片看好，能够复现比特币的辉煌，随后我记得是马老师的一句话，对就是提前退休的那个马老师，大概意思是区块链是个好东西，比特币不是，接着开始大跌到底已经￥20左右，就被套了，就当交学费了。今年也是第二季度忘了哪个月EOS回涨了，到￥53想都没想马上脱坑，及时止损 不经历一下真的很难感受到人的欲望是无限的，当时在最高点的心态现在想想都后怕，还好及时止损了没亏多少，但也买了教训，期间看到各种人梭哈，想着翻盘不顾一切的那种，最后被套就很恐怖，和赌博很像，经历过就深有体会，关爱生命远离币圈，哈哈 西北大环线 拿着EOS脱坑的钱我换了一身行头，和同学去了西北大环线，青海湖，大柴旦，翡翠湖，最美公路，魔鬼城，莫高窟，丹霞，祁连大草原这些地方（都忘的差不多了，看微博想起来的），感受了祖国的大好河山 我的梦想就是能够和另一半读万卷书，行万里路，看遍天下的奇闻异事，这次是4个同学一起还都是男的，也算完成了一点点梦想吧 毕业 研究生阶段让我收获最大的是鱼人沟通和思考问题的方式，从开始的愣头青变成了会思考的愣头青；简单总结一下研究生的生活吧，心态从抱怨-&gt;无望-&gt;庆幸，抱怨开始工作学习太多被压着喘不过气，无望是看不到自己毕业的希望以出路-&gt;庆幸自己能够到这个实验室遇见最好的各位，也正是和优秀的各位一起学习和努力，顺利毕业，找到满意的工作，也有了新的规划 毕业前我还玩了一个月的尤克里里，会了几个和弦，能弹两个简单的曲子不过现在忘了，还有个小插曲马哥分手了，给我打电话嚎啕大哭，7年的感情说没就没，周哥说就像满级的号被盗了一样，我瞬间感受到了马哥的痛苦，哈哈；按照实验室的传统，毕业三顿饭，吃一顿少一顿，今年由于不可抗力因素变成两顿，最后一顿也是玩的非常开心，也说了我一直想说的话，即使没有结果，起码没留遗憾，之后我便回家呆了几天就去入职了 入职 入职前，我一直关注脉脉，都是职场环境不好的言论，各种甩锅，扯皮，明争暗斗，看的我十分害怕，给自己定的基调就是多听多看少说话，保证不出错，进了这个部门非常和谐，所有人都很nice，我的问题所有都会被解答，非常庆幸能够加入这个团队 到现在入职已有半年了，有过几次培训认识了很多人，发现身边大多都是97年以后的，很多活动自己已经提不起兴趣了，而他们还激情满满，暗暗感叹自己心态老了,中秋节是个转折点我有了新的收获 技术上自己成长了很多，这段时间刷新了对很多知识的理解，遇到的所有知识点都写下来，到现在为止已经写了26篇博客，目前来看都是偏应用和概念性的，大多都是扫盲，之前没有接触过这些东西，只能先了解一个大概，到后期再细细钻研 总结我的2019—-&gt;有自己的小确幸 2020年继续加油，目标（暂时这么些吧，刚毕业还是积累为主） 每月平均两篇博客的更新 把19年欠下的博客补上：spring/spring boot、WAL、设计模式 netty 把部门核心业务吃透，核心产品源码看完，最好能开始学习下一代微服务学习 一次5天以上旅行 奥利给~~]]></content>
      <categories>
        <category>生活杂谈</category>
      </categories>
      <tags>
        <tag>生活杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019的最后一天踩坑正则表达式]]></title>
    <url>%2F2019%2F12%2F31%2F2019%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%A4%A9%E8%B8%A9%E5%9D%91%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[背景2019年的最后一天，和往常一样高高兴兴写着bug，遇到一个了需求 需要判断字符串前后是否有空白，返回true和false即可 思路正则表达式呗（条件反射的方案） 做法 第一步，打开浏览器用Google百度一下判断字符前后空白的正则 /(^\s)|(\s$)/g （看了一眼没毛病） 第二步，command+c出来 第三步，编写代码，把正则command+v过去 1Pattern.matches(&quot;/(^\\s*)|(\\s*$)/g&quot;, ” test “) 第四步，验证，run之后，返回false，嗯，bug来了 挣扎是我拷贝的姿势不对吗？换姿势试试 (^\s)|(\s$) ^\s|\s$ ^\s* \s*$ 结果还是false，团灭 字符串改成 ” “，返回true了，emmm…想不到了 看看源码是怎么说的，Pattern的matches方法底层调用的时Matcher的matches方法，先看注释 Attempts to match the entire region against the pattern. entire-&gt;整个的，意思是说matches方法是会匹配整个字符串 解决知道原因了，正则不对，网上的正则是匹配字符串前后的空白字符，并不是整个一起匹配，所以得重写正则 复制的正则 /(^\s)|(\s$)/g 修改后正则 (^\s.)|(.\s$) 试一下，所有都能匹配 是 * 的问题 修改一下 (^\s+.)|(.\s+$) 完美 中间补了一下正则表达式 五分钟搞定正则表达式，如果没搞定，再加两分钟]]></content>
  </entry>
  <entry>
    <title><![CDATA[TCP]]></title>
    <url>%2F2019%2F12%2F26%2FTCP%2F</url>
    <content type="text"><![CDATA[TCP报文 源端口 16bit 目的端口 16bit 序号 32bit seq 发送SYN时（SYN控制位是1）会初始化序列码（Initial Sequence Number, ISN），会用一个算法生产随机数 确认序号 32bit ack 伴随ACK（ACK控制位是1）报文一起，表示下一个要接收包的序列开始 4位首部长度 4bit 保留 6bit 标志 6bit URG 紧急标志 ACK 应答标志 PSH 推 RST 重置连接标志 用于重置连接 SYN 同步标志 用于建立连接 FIN 完成数据发送标志 用于释放连接 窗口大小 16bit 校验和 16bit 紧急指针 16bit 三次握手 客户端向服务器发送请求，等待服务器响应，客户端进入SYN_SENT状态；SYN=1, seq=x 服务器收到客户端建立连接的请求（SYN=1表示建立连接），向客户端发送响应，随后服务器进入SYN_RCVD状态；SYN=1，ACK=1，ack=x+1，seq=y 客户端收到响应确认后（确认ack=x+1，表示服务器接收到x+1前的所有的数据，没有丢包），进入到ESTAPLISHED状态（表示客户端向服务器发送的数据x可以被接收到，单方向连通），再向服务器发送响应，服务器接收到响应确认后（确认ack=y+1，表示接收到y+1前的所有的数据，没有丢包）进入ESTABLISHED状态（表示服务器想客户端发送的数据y可以被接收到，单方向连通）；ACK=1 ack=y+1 SYN=1 表示建立连接的请求 ACK=1 表示确认收到请求 seq 表示初始序列，相当于发送数据的开始索引 ack 表示收到数据后的序列，相当于下一次发送数据的开始索引（比如 ack=x+1 表示x+1前的数据接收到了，下次发送从x+1开始） 三次握手其实就是为了建立连接，客户端&lt;——&gt;服务器两个方向，一次连接建立的过程必须是发送seq序列并收到ack=seq+1的确认 为什么不是2次握手或者是4次握手 2次握手；当客户端发送了SYN=1 seq=x的连接请求，服务器返回ack=x+1的响应，如果是两次握手此时已经建立连接；这时候只能保证客户端向服务器发送的数据能被成功接收，而不能确定服务器给客户端发送的数据是否能被接收，相当于只是建立了 客户端——&gt;服务器单方向的连接 4次握手，参照2次握手发现建立单方向的连接往往需要2次握手，两个方向建立必然会4次握手，其实是第二次握手做了两件事，一是收到服务器的响应并确认，二是发送想服务器建立连接的请求；这两件事一起做相当于是3次握手 TCP不会重传ACK=1的报文，只能通过重发SYN=1的请求来尝试重新建立连接，比如（A是客户端，B是服务端） 第一个包，即A发给B的SYN没有到达B，A会超时重传，直到收到B的确认 第二个包，即B发送给A的SYN+ACK没有到达A，B会超时重传，直到收到A的确认 这里的报文虽然是ACK但是也包含SYN，所以可以重传 第三个包，即A发送给B的ACK没有到达B a.如双方果没有数据发送，B会超时重传，知道收到A的确认 b.如果A有数据发送，因为第二个包已经成功接收，A为ESTABLISHED状态，A-&gt;B的连接已经建立，A会发送Data+ACK的确认，当B接收到的时候会改变状态为ESTABLISHED c.如果B有数据发送，由于B没有收到确认还不是ESTABLISHED状态，还不能发送数据，会一直周期性超时重传SYN + ACK，直到收到A的确认才可以发送数据 TCP会不会重复建立连接呢？ 场景：客户端A和服务器B建立连接，第一个连接请求a没有收到，重新发送第二个请求b，请求b被接收后建立连接，过一会B又收到请求a 答案：不会，服务器B接收到请求a，认为a是无效请求，返回rst报文拒绝连接 假设如果已经建立了连接，服务器B发现请求a是失效的（因为是重发的请求源IP、端口和目标IP、端口是一样的，此时端口已经打开，连接建立）并向客户端A返回rst包，拒绝连接； 假设还连接还没建立，服务器B向客户端A返回ACK+SYN，A收到后验证发现该请求是无效的，回复rst关闭连接，B收不到回复会重发ACK+SYN到一定次数（可设置）不再重发并关闭这个未建立起来的连接 四次挥手 客户端发送向服务器发送关闭请求（FIN=1，seq=x），请求关闭客户端——&gt;服务器的连接，客户端进入FIN_WAIT_1的状态；客户端停止向服务器发送数据 服务器接收到关闭请求后，回复（ACK=1 ack=x+1 seq=y）给客户端，表示接收到关闭请求，服务器进入到CLOSE_WAIT状态，客户端收到回复后进入FIN_WAIT_2的状态 服务器发送向客户端发送关闭请求（FIN=1，ACK=1，seq=z，ack=x+1），服务器进入LAST_ACK状态；服务端停止向客户端发送数据，等待客户端响应 客户收到服务端的关闭请求，回复（ACK=1，seq=x+1，ack=z+1），客户端进入TIME_WAIT状态，等待2MSL（报文的最大生存时间）时间后，客户端进入CLOSED状态，服务器收到客户端的响应后进入CLOSED状态 等待2MSL的作用 1.为了保证客户端发送的ACK能够到达服务器，服务器的连接能够正常关闭； 假设ACK丢失，等待1个MSL的时间服务器重新发送FIN关闭连接，客户端在1个MSL的时间内收到重发的FIN请求就知道刚刚的ACK没有送到，重新发送ACK确认并重新计算MSL的时间；假设第四次挥手后直接关闭，服务端没有收到ACK重发FIN，此时客户端发送rst报文关闭连接，这时是异常的关闭，太暴力了不优雅 2.可以防止已经失效的数据包在下次的连接中传输 失效的数据包由于网络延迟，还未发送到目的地，这时数据包虽然已经生效但是生命周期还没有结束（在MSL的时间内），假如第四次挥手后马上关闭连接，此时又有新的连接建立，刚好是相同的IP和端口，旧的数据包传输可以在MSL时间内传输，等待2MSL后所有失效的数据包都已不存在 滑动窗口用来加速数据传输，假设A发送序列seq=x的包，必须等待收到ack=x+1的回复才继续发送后面的包，滑动窗口相当于规定了一个范围，只要发送seq的范围没有超过滑动窗口就能继续发送；这样发送端不需要长时间等待前一个ack就能继续发送后面的数据包，接收端可以收到多个数据包后只发送一个ack来表示确认，加速了传输速度 SYN攻击发生在三次握手的第三次过程中，服务器还没接收到客户端ack，状态是SYN_RCVD的时间段中，客户端伪造大量不存在的IP的SYN包，请求建立连接，服务器回复并等待，源ip是伪造的并不存在，服务器一直超时重发，造成网络堵塞，检测SYN攻击的方式很简单，服务器上有大量SYN_RCVD状态的链接，并且IP地址是随机的，可以用命令（#netstat -nap | grep SYN_RECV）判断 RST攻击客户端A与服务器B已经建立连接，C伪装成A发送RST包，B接收到后强制断开连接；亦或者C伪装A发送SYN包，B接收后发现该请求无效返回RST包要A断开连接；所以客户端的端口设置是随机的，不然很容易被猜到从而受到攻击 长连接短连接 短连接，客户端和服务器完成一次请求和响应，相当于完成一次读写，一般由客户端发起关闭连接操作，（服务器收到消息后关闭连接不优雅）；优点是便于管理，存活的连接都是有用的连接 长连接，客户端和服务器的连接不会主动关闭，后续的请求响应继续使用这个连接，通过保活机制维护连(2小时内没有请求和响应，服务器会想客户端发送一个探测报文) 客户端响应正常，时间刷新，2小时没有操作后继续探活 客户端不能响应探测报文（客户端异常无法发送响应或发送的响应无法到达服务器），75s后会超时，服务器共发送10个这样的探测，间隔75秒，10次结束后仍然没有收到回应，关闭连接 半连接队列和全连接队列 指的是服务器的状态，SYN_RCVD状态的连接会加入到半连接队列，服务器收到客户端的确认报文状态改为ESTABLISHED状态会从半连接队列中删除，加入到全连接队列 半连接队列满 不开启net.ipv4.tcp_syncookies，直接丢弃新来的SYN请求 开启net.ipv4.tcp_cookies，假设全连接队列满，并且qlen_young的值（半连接队列中还没有进行SYN+ACK的连接数量）大于1，丢弃这个SYN请求；假设全连接队列没满，生产syncookie并返回SYN+ACK包 syncookies用来防止syn floods攻击（攻击方不停发送SYN请求，不去回应ACK，使得半连接队列满，其他连接无法建立），通过将接收到的源ip源端口序列号进行hash，称为hash值，将hash值作为seq发送SYN+ACK，收到ACK响应验证cookie是否正确（ack-1），正确才能建立连接；但是对于没有受到攻击的服务器来说syncookies会造成负担 假设全连接队列满，会根据tcp_abort_on_overflow的值，执行相应的策略（值为0，服务器丢弃该连接，连接信息仍然保留在半连接队列中，服务器会重发SYN+ACK，直到队列不满，建立连接；值为1，服务器发送rst报文关闭连接） 拥塞控制控制网络流量，寻找一个合适的数据传输速度，防止造成网络堵塞或者传输速度过慢效率低的问题；拥塞窗口（cwnd）也就是发送数据的最大值，防止拥塞窗口过大，需要设置一个阈值（ssthresh）来控制窗口的大小，在阈值上下使用不同的算法 慢开始（cwnd &lt; ssthresh）；开始不会就发送大量的数据，由小到大慢慢增加拥塞窗口的大小，探测网络的拥塞程度，每收到一个ACK拥塞窗口（cwnd）增加一倍，从1开始（2的指数增加） 拥塞避免（cwnd &gt; ssthresh）；让窗口慢慢增大，经过一个往返时间RTT（收到一个ACK），窗口大小加1 在慢启动和拥塞避免的阶段中当出现网络拥塞（有报文超时），窗口会重置为1，ssthresh的阈值会重置为发生拥塞时窗口的一半，再重新开始传输数据 快重传；发送方按顺序传递报文，当出现丢失数据，接收方会发送重复的确认告知发送方报文丢失，发送方收到三个重复的确认立即重发报文，不必等待报文超时再重传 快恢复；发送方收到3个重发确认，将ssthresh减半，不执行慢开始（因为收到重复确认所以此时网络并没有拥塞，执行拥塞避免窗口加1，缓慢增大即可） 参考资料TCP为什么是三次握手，而不是两次或四次 TCP建立连接时三次握手的一个疑问点 TCP中的RST标志(Reset)详解 TCP协议详解 TCP流量控制、拥塞控制]]></content>
  </entry>
  <entry>
    <title><![CDATA[阻塞非阻塞，同步异步，网络I/O模型概念]]></title>
    <url>%2F2019%2F12%2F20%2FI-O%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[I/O 进程或线程会产生I/O，后面都以线程为例，真正操作I/O的是操作系统，也就是说线程线程向操作系统发送I/O请求，由操作系统来完成I/O执行，整个过程为应用程序的I/O调用； I/O调用的过程就是将进程中（用户空间）的数据输出到进程外部的的空间（系统空间），或者是将进程外部空间（系统空间）的数据输入到进程中（用户空间）；例如一个输入类型的I/O调用，线程首先向操作系统请求外部数据，操作系统将外部数据拷贝到内核缓冲区，进程中的线程再将内核缓冲区的数据拷贝到进程缓冲区，线程针对这部分数据继续后面的操作； 一个线程发出I/O请求后，需要等待I/O数据就绪（操作系统将I/O数据从外部拷贝到系统空间） 阻塞、非阻塞、同步、异步阻塞非阻塞 阻塞和非阻塞；等待I/O数据就绪是否可以做其他操作；一个线程请求I/O并且I/O数据未就绪，如果线程会一直等待不会做其他事情这种方式为阻塞，如果线程立即收到I/O数据未就绪的返回值，并不需要一直等待这个方式为非阻塞，通常是轮询去访问I/O数据是否就绪，虽然没有一直等也是需要不断去询问； 同步异步 同步和异步；可以类比线程的同步和异步，同步必须拿到I/O数据才能进行后面的操作，有很强顺序性，没有I/O数据就不能完成后面的操作，而异步不需要拿到I/O数据，去做别的操作没有顺序性；或者我觉得是不是可以这么理解（I/O数据从内核空间到用户空间拷贝是用户线程还是内核线程；同步在I/O数据就绪后用户线程自己将数据拷贝到用户空间；异步不关心I/O数据是否就绪，只是发去I/O请求有内核线程将数据拷贝到用户空间再通知给用户线程） 我来理解一下：简单来说同步异步的区别在于是否需要很强的顺序性；就是说同步异步区别在于用户线程是否需要拿到I/O数据再进行后面的操作，同步必须要用到这部分数据，I/O数据没有就绪就一直会等待，而异步不需要用到这部分数据，仅仅发送I/O请求等待操作系统通知即可，（这里同样提了等待，和阻塞的等待差不多都是等待I/O数据是否就绪，同步异步的关注点是是否需要等待I/O数据就绪完成下面的操作，类比于线程的同步和异步；而阻塞的关注点是等待I/O数据就绪的过程是否是一直在死等还是在做其他操作，非阻塞就是没有在死等，这段时间可以做其他的操作但是通常我们都会不断去询问I/O数据是否就绪） 总结：上面同步异步给了两种不同的解释，我觉得不冲突都可以理解，看哪一种好理解就用哪一种吧，后续两种解释都会尝试说明一下，觉得混乱就看其中一种吧，括号里或者括号外 组合概念 同步阻塞；线程发起I/O请求，I/O数据未就绪线程等待，这是阻塞，拿到I/O数据之后才进行进一步操作（I/O数据就绪后用户线程从系统空间拷贝数据到用户空间），这是同步；从程序的角度来看线程一直阻塞直到I/O数据就绪 同步非阻塞；线程发起I/O请求，I/O数据未就绪会立即收到一个返回值不用等待，这是非阻塞，拿到I/O数据之后才进行进一步操作（I/O数据就绪用户线程从系统空间拷贝数据到用户空间），这是同步；这里虽然不需要等待I/O数据就绪，但是由于是同步的，用户线程必须拿到I/O数据（数据需要用户线程来拷贝），此时由于I/O数据未就绪，用户线程无法对数据进行拷贝用户线程只能通过轮询的方式去询问I/O数据是否就绪，再进行下一步操作；从程序的角度来看线程只是卡在了等待I/O数据就绪这里，不会阻塞，此时可以去做其他的操作，只是通常是去做询问I/O数据是否就绪的操作 异步阻塞；其实这种情况是不存在的，异步和阻塞是矛盾的； 异步非阻塞；用户线程发起I/O请求后，无须关心I/O数据是否就绪，待I/O数据就绪后由操作系统将数据拷贝到用户空间，再向用户线程发送通知进行下一步操作；从程序的角度看现场不会阻塞 网络I/O模型 阻塞I/O 线程发起I/O请求会一直阻塞等待I/O条件就绪 非阻塞I/O 线程发起I/O请求后，如果I/O条件不是就绪状态立即返回一个状态不会一直等待，可以先做其他的任务，间隔一段时间查看I/O条件是否就绪，如果就绪进行下一步操作 多路复用I/O 非阻塞I/O线程需要一直去询问I/O事件是否就绪，如果线程很多必将造成资源的浪费；多路复用I/O将所有线程的I/O请求注册到一个新的线程中（select），由这个线程进行轮询去查看I/O条件是否就绪，有就绪状态就通知对应的线程进行处理；相当于是把非阻塞I/O中多线程查看I/O条件的事情委托给了单独的一个线程，提高了系统的吞吐量 信号驱动I/O 这个感觉和多路复用I/O差不多，这里将多线程的I/O操作注册为一个信号，信号中有回调函数，当信号发生call回调函数通知用户线程，与本节无关先简单这么理解 异步I/O 线程发出I/O请求后不需要做任何操作，I/O操作完全由操作系统内核完成，之后会通知线程]]></content>
  </entry>
  <entry>
    <title><![CDATA[回调与监听器模式]]></title>
    <url>%2F2019%2F11%2F22%2F%E5%9B%9E%E8%B0%83%E4%B8%8E%E7%9B%91%E5%90%AC%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[监听器模式是基于Java回调机制的，首先了解一下回调 回调类似于多线程中new Thread(){run()} 这样，其实run方法就是回调方法，jdk并不关注你怎么去实现run方法，将其以接口的方式暴露出来，让你去填空；就好像你的同事和你配合写代码，挖了一个坑让你填，哈哈，这么比喻有些不妥，其实是为了解耦，不想让你的代码侵入进去，直接看类图 类图 回调就包含两个部分，分别是调用者和回调接口，实现也很简单，调用者中维护回调接口的对象并增加set方法，调用者中直接使用接口中的方法，具体实现在调用set方法时填写即可，我可能没有说清楚，直接上代码 实现回调接口 Callback 123public interface Callback &#123; void doSomething();&#125; 调用者 Caller 1234567891011121314public class Caller &#123; // 维护一个回调接口 private Callback callback; public void setCallback(Callback callback) &#123; this.callback = callback; &#125; public void call() &#123; System.out.println(&quot;do something before&quot;); callback.doSomething();// 使用回调接口中的方法，不需要在这里关系实现 System.out.println(&quot;do something behind&quot;); &#125;&#125; 测试 123456789101112public static void main(String[] args) &#123; Caller caller = new Caller(); // 具体实现在set方法中填写 caller.setCallback(new Callback() &#123; @Override public void doSomething() &#123; System.out.println(&quot;do something in callback&quot;); &#125; &#125;); caller.call();&#125; 监听器模式监听器模式是监听感兴趣的事件，事件发生做出相应的操作；是回调的一种拓展，是在包括监听器接口，事件源和事件对象三个部分，先看类图 类图 相较于回调，监听器接口相当于回调方法接口，事件源相当于调用者，监听器模式是在此基础上多了事件对象，并传给接口的方法中，我的理解是事件对象是对事件的一个封装，感兴趣的事件可能有多个，可以针对不同事件（不同的event对象）做不同的操作，这些操作也被封装在不同event对象中 实现监听器接口 123public interface EventListener &#123; void doSomething(Event event);&#125; 事件源 123456789101112131415public class EventSource &#123; // 维护监听器对象 private EventListener listener; public void setListener(EventListener listener) &#123; this.listener = listener; &#125; public void eventHappend(Event event) &#123; System.out.println(&quot;do something before&quot;); listener.doSomething(event);// 不关心方法具体实现，并传入event对象参数 System.out.println(&quot;do something behind&quot;); &#125;&#125; 事件对象 12345678910111213141516public class Event &#123; // 事件类型 private String eventInfo; public Event(String eventInfo)&#123; this.eventInfo = eventInfo; &#125; public String getEventInfo() &#123; return eventInfo; &#125; // 对于事件的操作 void doSomething() &#123; System.out.println(&quot;do something in event object&quot;); &#125;&#125; 测试 123456789101112131415public static void main(String[] args) &#123; EventSource eventSource = new EventSource(); eventSource.setListener(new EventListener() &#123; @Override public void doSomething(Event event) &#123; event.doSomething();// 不管事件是什么直接执行方法 // 有感兴趣的事件做对应的操作 if (&quot;event interested&quot;.equals(event.getEventInfo())) &#123; System.out.println(&quot;interesting event happened&quot;); &#125; &#125; &#125;); eventSource.eventHappend(new Event(&quot;event interested&quot;));&#125; 感觉事件对象还没有理解到位，针对不同的事件做不同的操作，可以设置不同的监听器，在不同的监听器中做对应的操作，相当于回调不用封装时间对象；也可以封装不同的监听事件作为参数传入，唯一的监听器中，针对不同的事件对象再做不同的操作。]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Explain记录]]></title>
    <url>%2F2019%2F11%2F16%2FSQL-Explain%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[要优化SQL必须得使用Explain，这里记录一下Explain的使用，直接查看即可，摘自MySQL Explain详解 explain 直接加载SQL语句之前，然后一起执行即可，只能分析查询语句，会出现以下结果 IDselect的查询顺序标识，SQL顺序由大到小执行，id相同从上往下顺序执行 select_typeselect的类型 SIMPLE(简单SELECT，不使用UNION或子查询等) PRIMARY(子查询中最外层查询，查询中若包含任何复杂的子部分，最外层的select被标记为PRIMARY) UNION(UNION中的第二个或后面的SELECT语句) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询) UNION RESULT(UNION的结果，union语句中第二个select开始后面所有select) SUBQUERY(子查询中的第一个SELECT，结果不依赖于外部查询) DEPENDENT SUBQUERY(子查询中的第一个SELECT，依赖于外部查询) DERIVED(派生表的SELECT, FROM子句的子查询) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行) table显示这一步所访问数据库中表名称（显示这一行的数据是关于哪张表的），有时不是真实的表名字，可能是简称，例如上面的e，d，也可能是第几步执行的结果的简称 type对表访问方式，表示MySQL在表中找到所需行的方式，又称“访问类型”。 常用的类型有： ALL、index、range、 ref、eq_ref、const、system、NULL（从左到右，性能从差到好） ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行 index: Full Index Scan，index与ALL区别为index类型只遍历索引树 range:只检索给定范围的行，使用一个索引来选择行 ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 const、system: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量，system是const类型的特例，当查询的表只有一行的情况下，使用system NULL: MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 possible_keys指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用（该查询可以利用的索引，如果没有任何索引显示 null） 该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询 Keykey列显示MySQL实际决定使用的键（索引），必然包含在possible_keys中 如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 key_len表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的） 不损失精确性的情况下，长度越短越好 ref列与索引的比较，表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows估算出结果集行数，表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 Extra该列包含MySQL解决查询的详细信息,有以下几种情况： Using where:不用读取表中所有信息，仅通过索引就可以获取所需数据，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤 Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询，常见 group by ; order by Using filesort：当Query中包含 order by 操作，而且无法利用索引完成的排序操作称为“文件排序” Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 Impossible where：这个值强调了where语句会导致没有符合条件的行（通过收集统计信息不可能存在结果）。 Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行 No tables used：Query语句中使用from dual 或不含任何from子句 总结： EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况 EXPLAIN不考虑各种Cache EXPLAIN不能显示MySQL在执行查询时所作的优化工作 部分统计信息是估算的，并非精确值 EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。 重点关注：type至少达到range级别 key列有值，并且key_len越少越好，做到有索引的查询 rows列越少越好]]></content>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL级联查询一些总结]]></title>
    <url>%2F2019%2F11%2F16%2FSQL%2F</url>
    <content type="text"><![CDATA[子查询导致索引失效 连接查询（连接条件为索引）的效率更高 背景：微服务相关管理端的系统，用户会在自己对应服务的地方查询所需的server，新增需求为要查看自己的服务所在的app 数据结构：server在单独的一张表，可以根据服务名称（interface_name）来查询；AppName在另外的一张表中；两张表没有联系需要通过一个中间表来连接；interface_name有索引，三个表之间的链接字段都有索引 分析：需要联合3张表来查询所需要的数据，每张表的数据量都比较大，而且这个SQL是系统使用最频繁的部分查询的频率还特别高，所以要尽可能快的出结果 我的心路历程：先通过interface_name条件筛选出一部分数据再链接另外两张表查询，都有索引一定是最优的，SQL如下 1234567891011SELECT app_name FROM saf_app WHERE app_id IN ( SELECT DISTINCT app_id FROM saf_ins_hb WHERE ins_key IN (SELECT ins_key FROM saf_server WHERE interface_name = &apos;xxx&apos;) ) ORDER BY app._name 查询时间竟然需要6s多，这绝对是不能忍的，接着我又试了一下级联查询，SQL如下 123456SELECT DISTINCT app.app_name FROM saf_server s LEFT JOIN saf_ins_hb hb ON s.ins_key = hb.ins_keyLEFT JOIN saf_app app ON hb.app_id = app.app_id WHERE s.interface_name = &apos;xxx&apos; ORDER BY app.app_name 这次的结果只需0.05s,相差100倍还多 explain看下呢 IN子查询如下 可以看到id为2和3的查询都用到了索引并且只需扫描的很少的行数，到了最外层的查询就变成了全表扫描了，索引就失效了 级联查询如下 级联查询全部使用到了索引，而且扫描的行数比子查询的要少很多，扫描的最终行数是乘积的关系，级联查询有两个子查询的rows为1所以要比IN子查询要小很多 所以说IN子查询会导致部分索引失效，我有了新的想法，既然连接查询会很快那么我先通过条件筛选出数据再做级联查询不是更快了，开整~ SQL如下 12345SELECT DISTINCT app.app_name FROM (SELECT ins_key FROM saf_server WHERE interface_name = &apos;xxx&apos;) s LEFT JOIN saf_ins_hb hb ON s.ins_key = hb.ins_keyLEFT JOIN saf_app app ON hb.app_id = app.app_id ORDER BY app.app_name 查询0.02s左右，我非常满意，explain一下呢 相较于级联查询还多了7000多次的遍历？？？子查询害人啊，查询结果0.02s左右应该是有缓存 看了一篇文章说在on后面加限制条件会比where中加限制条件用时要少，on后面加条件在两张表做连接的同时过滤掉一些数据后再和第三张表做连接，where是将连接了所有表之后的结果进行筛选，听着很有道理，那我试一下呢，SQL如下 12345SELECT DISTINCT app.app_name FROM saf_server s LEFT JOIN saf_ins_hb hb ON s.ins_key = hb.ins_key and s.interface_name = &apos;xxx&apos;LEFT JOIN saf_app app ON hb.app_id = app.app_id ORDER BY app.app_name explain看下 结果非常意外，不仅时间没有省下来，索引也没有使用，进行全表扫描，还好我验证了一下，原因的话还不知道，对mysql底层不是很熟悉，先暂时把遇到的问题记录下来吧 ^_^ 结论：子查询会导致索引失效，尽量不使用子查询，用级联查询代替，并将级联查询的条件设置建立索引 级联查询的原理mysql会首先找到一张表作为驱动表，就是首先要进行查询的表，以驱动表为基础匹配剩下的表，inner join的情况mysql会选择数据量小的表作为驱动表，left/right join分别以左/右表作为驱动表；接着会根据on的条件过滤结果，最终将连接的表都筛选完成后如果有where语句指定条件将进行最后的筛选得到结果 连接的算法也很简单，连接条件没有索引则进行全表扫描然后进行匹配，如果还有表连接则将匹配的结果继续与剩余的表进行扫描匹配，这种方法简单粗暴，叫做嵌套循环连接（Nested-Loop Join）；Mysql对这种方式有了优化，增加了join buffer，是将驱动表关联条件的相关列缓存起来，并将多次匹配合并，减少的匹配的次数，以此方式来加速查询结果，叫做BLJ算法（Block Nested-Loop Join）；有索引则会先匹配索引，匹配后的结果再插到对应的数据返回 综上，级联查询的查询条件最好是加索引，虽然mysql对没有索引的链接做了优化，那也是没有索引的方式快的，而且最好链接的条件是主键索引，这是由于非主键索引指向的时主键索引，要得到数据还要跑一次主键索引；还有我想到了阿里巴巴java开发规范中写道多余三张表不能使用join，用多次简单查询代替这个也要注意一下 参考： MySQL查询优化——连接以及连接原理]]></content>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[踩坑记录]]></title>
    <url>%2F2019%2F11%2F06%2F%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[在xml文件中if条件判断字符串相等时要用双引号而不是单引号 错误栗子： 1&lt;if test=&quot;params.appName != null and params.appName != &apos;&apos; and params.appName != &apos;*&apos;&quot;&gt; 正确栗子： 1&lt;if test=&apos;params.appName != null and params.appName != &quot;&quot; and params.appName != &quot;*&quot;&apos;&gt; MyBatis会将’*’转化为数字，并且会报NumberFormatException 原因百度了一下大概是这样，MyBatis使用OGNL表达式来解析，在OGNL表达式中单引号和其中的字符会被解析成一个字符，java对于没有引号的等式/不等式认为是数字类型并进行转化]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven-assembly插件打zip包]]></title>
    <url>%2F2019%2F10%2F11%2Fmaven-assembly%E6%8F%92%E4%BB%B6%E6%89%93zip%E5%8C%85%2F</url>
    <content type="text"><![CDATA[web工程通过maven打包通常都是war包，Tomcat会自动将war包解压并发布出来，但如果本身做的不是web工程，是普通java项目如何发布到服务器上并运行main方法呢？公司里使用maven-assembly这个插件，将项目打包成zip压缩包，里面包含bin、conf和lib三个文件夹，bin目录中保存启动和停止的shell脚本，conf中保存配置文件，lib目录中保存编译好的jar和所依赖的jar；然后将zip包抽取并解压到服务器启动start.sh脚本来运行java项目。 在这个过程中就用到了maven-assembly这个插件来进行编译并打包，步骤如下 目录结构 1234567main |--assembly |----bin |---start.sh |---stop.sh |---jvm.properties |----assembly.xml pom中配置assembly插件 123456789101112131415161718192021&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;configuration&gt; &lt;!--打包规则的配置--&gt; &lt;descriptors&gt; &lt;descriptor&gt;src/main/assembly/assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;tarLongFileMode&gt;posix&lt;/tarLongFileMode&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 注：使用assembly插件编译要讲该插件的配置放在plugins标签中的第一个，在我的工程中开始在前面的时spring-boot-maven-plugin插件导致编译失败了 2.创建并配置assembly.xml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;assembly&gt; &lt;id&gt;assembly&lt;/id&gt; &lt;formats&gt; &lt;format&gt;zip&lt;/format&gt; &lt;format&gt;dir&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;!--输出文件的配置 3个属性分别是 编译路径 输出路径 文件权限--&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;outputDirectory&gt;conf&lt;/outputDirectory&gt; &lt;fileMode&gt;0644&lt;/fileMode&gt; &lt;/fileSet&gt; &lt;fileSet&gt; &lt;directory&gt;src/main/assembly/bin&lt;/directory&gt; &lt;outputDirectory&gt;bin&lt;/outputDirectory&gt; &lt;includes&gt; &lt;include&gt;start.sh&lt;/include&gt; &lt;include&gt;stop.sh&lt;/include&gt; &lt;/includes&gt; &lt;fileMode&gt;0755&lt;/fileMode&gt; &lt;/fileSet&gt; &lt;fileSet&gt; &lt;directory&gt;src/main/assembly/bin&lt;/directory&gt; &lt;outputDirectory&gt;bin&lt;/outputDirectory&gt; &lt;includes&gt; &lt;include&gt;jvm.properties&lt;/include&gt; &lt;/includes&gt; &lt;filtered&gt;true&lt;/filtered&gt; &lt;fileMode&gt;0644&lt;/fileMode&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;outputDirectory&gt;lib&lt;/outputDirectory&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt;&lt;/assembly&gt; 3.编写脚本 start.sh 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#!/bin/shBASEDIR=`dirname $0`/..BASEDIR=`(cd &quot;$BASEDIR&quot;; pwd)`echo current path:$BASEDIRBASEBIN_DIR=$BASEDIR&quot;/bin&quot;cd $BASEBIN_DIRLAF_REG_INSTANCE=&quot;test-jsf-demo&quot;LOGPATH=&quot;&quot;LAF_REG_PIDPATH=&quot;$BASEBIN_DIR&quot;if [ &quot;$1&quot; != &quot;&quot; ] &amp;&amp; [ &quot;$2&quot; != &quot;&quot; ]; then LAF_REG_INSTANCE=&quot;$1&quot; LOGPATH=&quot;$2&quot;fiif [ &quot;$3&quot; != &quot;&quot; ]; then LAF_REG_PIDPATH=&quot;$3&quot;fi# ------ check if server is already runningPIDFILE=$LAF_REG_PIDPATH&quot;/&quot;$LAF_REG_INSTANCE&quot;_startup.pid&quot;if [ -f $PIDFILE ]; then if kill -0 `cat $PIDFILE` &gt; /dev/null 2&gt;&amp;1; then echo server already running as process `cat $PIDFILE`. exit 0 fifi# ------ set JAVACMD# If a specific java binary isn&apos;t specified search for the standard &apos;java&apos; binaryif [ -z &quot;$JAVACMD&quot; ] ; then if [ -n &quot;$JAVA_HOME&quot; ] ; then if [ -x &quot;$JAVA_HOME/jre/sh/java&quot; ] ; then # IBM&apos;s JDK on AIX uses strange locations for the executables JAVACMD=&quot;$JAVA_HOME/jre/sh/java&quot; else JAVACMD=&quot;$JAVA_HOME/bin/java&quot; fi else JAVACMD=`which java` fifiif [ ! -x &quot;$JAVACMD&quot; ] ; then echo &quot;Error: JAVA_HOME is not defined correctly.&quot; echo &quot; We cannot execute $JAVACMD&quot; exit 1fi# ------ set CLASSPATHCLASSPATH=&quot;$BASEDIR&quot;/conf/:&quot;$BASEDIR&quot;/root/:&quot;$BASEDIR&quot;/lib/*echo &quot;$CLASSPATH&quot;# ------ set jvm memorysed &quot;s/\r$//g&quot; jvm.properties &gt; 1.propertiesmv 1.properties jvm.propertiesif [ -z &quot;$OPTS_MEMORY&quot; ] ; then OPTS_MEMORY=&quot;`sed -n &apos;1p&apos; jvm.properties`&quot;fiif [ &quot;`sed -n &apos;2p&apos; jvm.properties`&quot; != &quot;&quot; ] ; then JAVA_CMD=&quot;`sed -n &apos;2p&apos; jvm.properties`&quot; if [ -f $JAVA_CMD ]; then JAVACMD=$JAVA_CMD fifi#DEBUG_OPTS=&quot;-Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8000&quot;#JPDA_OPTS=&quot;-agentlib:jdwp=transport=dt_socket,address=8000,server=y,suspend=n&quot;# ------ run proxynohup &quot;$JAVACMD&quot; $JPDA_OPTS \ $OPTS_MEMORY $DEBUG_OPTS \ -classpath &quot;$CLASSPATH&quot; \ -Dbasedir=&quot;$BASEDIR&quot; \ -Dfile.encoding=&quot;UTF-8&quot; \ com.jd.testjsfdemo.TestjsfdemoApplication \ &gt; /Users/Logs/testjsfdemo_std.out &amp;# ------ wirte pid to fileif [ $? -eq 0 ]then if /bin/echo -n $! &gt; &quot;$PIDFILE&quot; then sleep 1 echo STARTED SUCCESS else echo FAILED TO WRITE PID exit 1 fi# tail -100f $LOGFILEelse echo SERVER DID NOT START exit 1fi stop.sh 1234567891011121314151617181920212223242526272829303132#!/bin/shBASEDIR=`dirname $0`BASEDIR=`(cd &quot;$BASEDIR&quot;; pwd)`echo current path $BASEDIRLAF_REG_INSTANCE=&quot;test-jsf-demo&quot;LAF_REG_PIDPATH=&quot;$BASEDIR&quot;if [ &quot;$1&quot; != &quot;&quot; ]; then LAF_REG_INSTANCE=&quot;$1&quot;fiif [ &quot;$2&quot; != &quot;&quot; ]; then LAF_REG_PIDPATH=&quot;$2&quot;fiPIDFILE=$LAF_REG_PIDPATH&quot;/&quot;$LAF_REG_INSTANCE&quot;_startup.pid&quot;echo $PIDFILEif [ ! -f &quot;$PIDFILE&quot; ]then echo &quot;no registry to stop (could not find file $PIDFILE)&quot;else kill $(cat &quot;$PIDFILE&quot;) sleep 10 kill -9 $(cat &quot;$PIDFILE&quot;) rm -f &quot;$PIDFILE&quot; echo STOPPEDfiexit 0echo stop finished. jvm.properties 12-Xms1024m -Xmx1024m -Xmn400m/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/bin/java 4.编译后就成功啦，之后在jdos上配置一下就可以自动部署了]]></content>
  </entry>
  <entry>
    <title><![CDATA[观察者模式]]></title>
    <url>%2F2019%2F09%2F19%2F%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[观察者模式包含观察者和被观察者两个部分，原理也很简单，被观察者类中维护观察者对象的集合，当感兴趣的事件发生，遍历观察者的集合回调观察者对象中的相应方法（update）即可 被观察者除了维护观察者的集合外，还有对该集合的增（注册）、删（取消）及通知等操作 实现观察者123public interface Observer &#123; update(String msg);&#125; 1234567891011public class ObserverImpl implements Observer &#123; private String name = &quot;&quot;; Observer(String name)&#123; this.name = name; &#125; @override public void update(String msg)&#123; // 感兴趣的事件发生 System.out.println(msg); &#125;&#125; 被观察者123456public interface Observed&#123; registerObserver(Observer observer); removeObserver(Observer observer); notify(Observer observer); notifyAll();&#125; 1234567891011121314151617181920212223public class ObservedImpl implements Observed&#123; private static List&lt;Observer&gt; list = new ArrayList&lt;&gt;(); @Override public void registerObserver(Observer observer)&#123; list.add(observer); &#125; @Override public void removeObserver(Observer observer)&#123; list.remove(Observer); &#125; @Override public void notify(Observer observer)&#123; list.stream().filter(n -&gt; n.equals(observer)).update(&quot;notify&quot;); &#125; @Override public void notifyAll()&#123; list.stream().map(n -&gt; n.update(&quot;notifyAll&quot;)); &#125;&#125; 测试123456Observed o = new ObservedImpl();Observer o1 = new ObserverImpl(&quot;o1&quot;);Observer o2 = new ObserverImpl(&quot;o2&quot;);o.registerObserver(o1);o.registerObserver(o2);o.notifyAll(); Observer 和 Observablejdk中提供了观察者模式的API，java.util包下的Observer接口和Observable类，原理是一样的，这里只是进行的封装 上源码 Observer123public interface Observer &#123; void update(Observable o, Object arg);&#125; 和上面观察者的部分一样，实现一个回调方法，当感兴趣的事件发生回调该方法 使用时，实现Observer接口，重写update方法即可，当感兴趣的事件发生会回调update方法，这里会写增加的业务逻辑 Observable12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class Observable &#123; private boolean changed = false; private Vector&lt;Observer&gt; obs; public Observable() &#123; obs = new Vector&lt;&gt;(); &#125; public synchronized void addObserver(Observer o) &#123; if (o == null) throw new NullPointerException(); if (!obs.contains(o)) &#123; obs.addElement(o); &#125; &#125; public synchronized void deleteObserver(Observer o) &#123; obs.removeElement(o); &#125; public void notifyObservers() &#123; notifyObservers(null); &#125; public void notifyObservers(Object arg) &#123; Object[] arrLocal; synchronized (this) &#123; if (!changed) return; arrLocal = obs.toArray(); clearChanged(); &#125; for (int i = arrLocal.length-1; i&gt;=0; i--) ((Observer)arrLocal[i]).update(this, arg); &#125; public synchronized void deleteObservers() &#123; obs.removeAllElements(); &#125; protected synchronized void setChanged() &#123; changed = true; &#125; protected synchronized void clearChanged() &#123; changed = false; &#125; public synchronized boolean hasChanged() &#123; return changed; &#125; public synchronized int countObservers() &#123; return obs.size(); &#125;&#125; 同样，这里维护一个观察者的集合Vector，这里考虑了线程安全的问题，说明这种方式实现的观察者模式是线程安全的 除此之外还有一个bool类型的变量changed表示被观察者是否发生改变（也就是感兴趣的事件是否发生），通过该标志来通知观察者对象 同样，该类里有针对观察者集合的增、删、通知的操作，还多了对changed标志修改的操作；除此之外所有方法都有synchronized关键字，进一步说明了这种方式的观察者模式是线程安全的 使用时，在要被观察的类中继承Observable类，再添加实现了Observer接口的观察者对象，调用setChanged()方法改变changed标志后通过调用notify()方法进行通知]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper扫盲]]></title>
    <url>%2F2019%2F08%2F29%2FZookeeper%E6%89%AB%E7%9B%B2%2F</url>
    <content type="text"><![CDATA[Zookeeper是一个分布式协调服务框架，这句话对于刚接触ZK（Zookeeper下文简称ZK）的我来说太抽象了，很难理解，只知道它能提供一些服务能够实现配置管理、命名服务、分布式锁等等，也就是在这些场景下会使用到ZK，这样说好像还是很难理解；之后，我找到了一个比较好理解的点，就是从CAP理论的角度，这需要对CAP有些了解，提前做了功课(CAP和BASE)，简单来说在分布式系统中出现网络故障时，最多满足其中的两项，而P是必须要满足的，那么就需要从CP和AP中做选择了；ZK就是可保证CP的框架，最后我的理解就是ZK通过特定的数据结构，封装一系列算法通过API的方式提供分布式环境数据一致性的服务，所有需要数据一致性的场景都可以使用ZK，也就是上面提到的配置管理、命名服务、分布式锁等等场景 入门数据结构ZK提供了一套类似于文件目录的数据结构，叫做多层级的节点命名空间，每个节点（ZK中称为znode）都可以存放数据且每个节点下都有若干个子节点，听起来和树的结构差不多，其实也一样，目录也是一种树结构的实现，znode可以理解为文件夹，文件夹中可以存放文件（znode可以存放数据），也可以存放文件夹（znode也可以存放znode） znode的类型 PERSISTENT–持久化目录节点 客户端和ZK断开连接后节点依然存在 PERSISTENT_SEQUENTIAL–持久化顺序编号目录节点 与持久化目录节点相同，只是多了ZK对节点的顺序编号 EPHEMERAL–临时目录节点 客户端和ZK断开连接后节点被删除 EPHEMERAL_SEQUENTIAL–临时顺序编号目录节点 与临时目录节点相同，只是多了ZK对节点的顺序编号 通知机制ZK还提供了类似于观察这模式的通知机制，称为watcher事件，可以观察到znode的变化，来通知客户端，之后客户端再做相应的业务逻辑 使用场景命名服务这里命名服务指的是通过指定名字获取对应的资源，将资源存储在特定路径的znode中，根据路径就可以找到资源，类比目录结构来说，拿到文件的地址就能通过地址来找到文件，有点像是URL的意思，但是由于ZK数据结构设计的因素ZK不能存放较大的数据；微服务框架中的注册中心需要存放provider和consumer的信息，并且consumer要能够感知provider的实时状态，ZK可以根据provider和consumer的地址映射成临时znode结构，这样既保存了provider和consumer的信息还能感知彼此的状态 配置管理一句话解释—-动态下发配置文件变化；通过ZK客户端watch配置文件，一旦配置文件发生变化马上通知客户端做对应的处理 分布式锁多个客户端再ZK的同一个目录下尝试创建临时znode，成功创建znode意味着获得锁成功，下个客户端发现目录下已经存在znode则对该znode添加watch机制，当znode消失即为释放锁后，通知客户端尝试创建znode来获取锁，这是公平锁；非公平锁的则创建临时有序的znode，相当于一个队列，后面的节点watch前一个节点的znode的状态，队头的znode为获得锁成功的几点 选举算法ZK节点的状态 LOOKING–当前节点不知道leader是谁，正在搜索 LEADING–当前节点为集群的leader FOLLOWING–目前已有leader，当前节点负责与leader节点同步 选举过程在两种情况下会进行选举，1.服务器初始化启动 2.集群中leader节点故障 起初集群中的节点没有leader或者不知道leader是谁，此时节点的状态为LOOKING，如果当前集群存在leader（该节点新加入集群），此节点发送投票信息想要选举leader会被告知当前leader的信息，此节点只需和leader节点建立连接，并进行状态同步即可； 如果当前集群不存在leader节点，则需要投票进行选举 此时所有节点皆为LOOKING状态，并编辑投票信息发送给集群的其他节点，投票信息的格式为（SID,ZXID）（服务器的唯一标识，事务ID），SID是自己配置的，ZXID理解为当前节点数据的版本； 集群中的节点会受到其他节点的投票信息，加上自己的那一票会根据一个规则会投出第二轮的选票 节点会在自己收到的选票中，选择ZXID最大的作为第二轮的选票发送给集群中其他节点 如果ZXID相同则选择SID最大的作为第二轮的选票发送给集群中其他节点 集群中的节点在接收到第二轮选票后进行统计（包含自己的一票），获得集群中一半以上（&gt;n/2）数量投票的节点当选leader进入LEADING状态，其余节点进入FOLLOWING状态 总结一下，集群中要获得一半以上的投票才能当选leader，所以集群最少为3台，并且数量是奇数；集群中ZXID越大的节点（当前节点数据版本越新）优先当选leader 安装与启动整个过程没有难度也很好理解，跟着这篇文章做就完事了 最后本文是对于Zookeeper的扫盲，大致了解ZK的基本原理，为了更好的理解工作中的项目，具有目的性，一些细节没有去研究，下回再补，😋 参考资料ZooKeeper典型应用场景一览 笔记：Mac上zookeeper的安装与启动 zookeeper面试题]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAP和BASE]]></title>
    <url>%2F2019%2F08%2F26%2FCAP%E5%92%8CBASE%2F</url>
    <content type="text"><![CDATA[CAP理论Consistency（一致性） Availability（可用性） Partition tolerance（分区容错性） CAP理论的内容是在分布式系统中出现网络故障时最多只能满足CAP中的两项 Consistency 一致性 “all node see the same data at the same time” 就是指分布式系统的数据一致性，可用看出这里的一致性指的是强一致性，要求分布式系统数据发生改变后所有节点在后续的请求中都能感知到 强一致性，当数据发生改变，系统中的其他节点在下次请求都会感知到 弱一致性，保证某个时间级别（比如xx秒），数据能够达到数据一致的状态 最终一致性，弱一致性的一个特例，保证在一段时间内达到数据一致的状态 Availability 可用性 “Reads and Writes always succeed”，这里的succeed指的是请求和响应的过程成功，也每次请求都会在有限的时间内收到回复，换句话说就是服务器可用；并不是请求的返回值是成功的意思，返回值是失败也是有返回值的，同样说明请求响应的过程是成功的。 系统可用性5个9，意为分布式系统的可用水平为99.999%，全年停机时间不超过 (1-0.99999)36524*60 = 5.256 min Partition tolerance 分区容错性 “the system continues to operate despite arbitrary message loss or failure of part of the system” 分区指的是由于网络异常集群中只有部分节点能够正常通信，可能存在多个能够通信的子网络，这些子网络就是分区；又或者说集群中部分服务器宕机，其他服务器依然可用，这部分可用的服务器组成的子网络也可以称为分区。分区容错性是指当出现网络延迟或者故障的情况时系统依然能够提供服务 怎么理解呢？简单来说就是在分布式系统中发生故障时，CAP最多只能满足其中的两项，也就是CAP三选二呗，但是我们发现P（分区容错性）说白了就是指网络出问题后系统依然可用，这可是分布式系统的基础条件，应该必须满足；在满足分区容错性时有可能会存在两种问题 1.用户访问部分服务器间的网络异常，这时只需将请求转发到可用的服务器即可，这时在理想情况下（服务器间的网络通畅）是可以满足CAP的，除非和所有服务器的网络都有问题（这只是理想情况） 2.除了用户访问服务器的网络异常之外，分布式系统中不同服务器直接的网络也可能存在异常，例如分布式系统中有A、B两台服务器，假设A、B之间存在网络故障，当服务器A改变数据之后无法同步到服务器B，此时就不能保证强一致性和可用性同时满足 放弃强一致性（C），用户请求A可以得到最新的数据，用户请求B得到的是旧数据；可以保证用户每次请求都会返回结果，但不能保证数据的一致性 放弃可用性（A），为了保证数据的强一致性，数据每次修改后都需要等待所有的数据源都同步后才能进行读写，用户请求服务器A或者B，由于数据始终不能同步，最后会一直阻塞下去，不能保证用户的每次都能短时间内得到返回值甚至得不到返回值 综上，我认为的CAP理论是在分布式系统中服务器之间网络出现问题时，CAP最多只能满足其二，并不是分布式系统就只能满足CAP中的两项，理想情况下是都可满足的（虽然现在不大可能）；况且在分布式系统中P是必须满足的，也就是说CA只能满足其一，具体的取舍需要根据不同的业务场景权衡 BASE理论 Basically Available（基本可用） 系统设计中可以牺牲部分可用性，比如允许响应时间增加1-2秒，服务降级等 Soft state（软状态） 允许系统中的数据存在中间状态，允许数据同步过程存在延迟 Eventually consistent（最终一致性） 所有数据再一段时间的数据同步后都能达到一致的状态 综上，不同的业务以BASE理论为基础对可用性和一致性进行一个权衡；zk和数据库的主从都是舍弃高可用性；涉及到用户体验的场景则需要舍弃数据强一致性如12306买火车票，618和双11等对于用户的每个请求都需要给与响应，允许存在短时间数据不一致的状态 参考资料]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存更新的套路 总结与感受]]></title>
    <url>%2F2019%2F08%2F21%2F%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0%E7%9A%84%E5%A5%97%E8%B7%AF-%E6%80%BB%E7%BB%93%E4%B8%8E%E6%84%9F%E5%8F%97%2F</url>
    <content type="text"><![CDATA[注： 本文是阅读 [缓存更新的套路] (https://coolshell.cn/articles/17416.html) 一文的总结，本文是以经常用到的Redis+MySQL的角度解读 经典方法 – Cache Aside Pattern 读操作：先从Redis中获取，没有命中则从MySQL中获取，最后更新到Redis中 写操作：先写MySQL，成功后删除Redis中的数据 为什么这样的方案会是经典方法呢？因为简单，有效，错误少 场景1–写操作如果先删除Redis中的数据再操作数据库 如果一个写操作和读操作并发执行，写操作删除了Redis中的数据还没来得及将数据更新到MySQL中，读操作没有在Redis中读取到数据，而从MySQL中读到旧的数据并更新到了Redis中，这样虽然MySQL中的数据是正确的，因为Redis的存在使得每次读取的数据都是脏数据。（这种情况只会发生在删除Redis数据后，写MySQL前这段时间，因为写MySQL时可以通过加悲观锁来避免问题） 场景2–写操作后增加更新Redis数据 这样读和写的操作都会更新Redis数据看似更稳健了，其实不然，更新操作变多了脏数据也会变多，比如有两个写操作a和b，按照请求的时间来说最终的状态应该是b，极端情况下ab都完成了更新数据库的操作，在更新Redis的时候，b先完成了，随后a完成又将Redis数据改成了a的状态，这样还是会存在脏数据 ，虽然出现的概率不大相对于经典方法多了产生脏数据的可能性，所以不可取 场景3–脏数据 经典方法可以避免场景1和2的问题，但也不是万无一失的，当一个读操作在Redis中没有命中时，从MySQL中获取了数据，在更新Redis数据之前，有一个写操作完成，此时MySQL的数据已经变了，而读操作会把之前的数据写入Redis中，产生脏数据。而这样的场景发生的几率非常非常小，由于MySQL锁的限制，只能发生在读操作读取数据之后读操作更新Redis数据之前，并且这段时间有一个写操作完成，同样因为锁的存在写操作一般都比读操作会耗时，并且给Redis数据增加过期时间进一步减小脏数据的产生几率 综上所述，虽然经典方法在场景3略有瑕疵，但依然经典可用，因为其简单，有效，错误少 Read/Write Through Pattern 读操作：Redis命中直接返回；没有命中从MySQL中读取后更新到Redis中 写操作：Redis中命中更新Redis；没有命中更新MySQL 这种模式写操作只针对于一个数据库（MySQL或者Redis），读数据时在Redis中没有命中会从MySQL中读取到Redis中，长时间运行后大部分数据都会在Redis中命中，写操作也会针对于Redis，相当于是强依赖于Redis，弱依赖甚至不依赖于MySQL，这么看来这种方式是最高效的，但是有一个致命的缺点，数据可能会丢失，由于强依赖于Redis中的数据，同样强依赖于Redis的高可用性，当Redis的数据丢失没有很好的灾备的话，数据就没了，这时如果想通过MySQL恢复几乎是不可能的了。总结一下，这种模式有利有弊，具体使用场景需要针对于具体的业务场景，我认为如果有比较好的Redis高可用和灾备的策略，这种模式还是非常好用的。 Write Behind Caching Pattern 读操作：Redis命中直接返回；没有命中则从MySQL中获取数据返回 写操作：无论Redis命中与否都先更新Redis，接着异步更新MySQL中的数据 原文中也提到，这么做会极大的提高I/O，相当于只操作了Redis，是内存级别的读写，同时这么做也有缺点，就是在同步数据到MySQL时如果服务器宕机断电等事故将会永久性的数据丢失 精彩评论摘取了一些大佬们的评论，很有启发 一位名叫 ty 的大佬说：”Cache Aside Pattern模式，两个更新操作同时进来，也可能会有cache脏数据的问题啊顺序如下：第一个写数据库，第二个写数据库，第二个写cache，第一个写cache这样cache里是第一个数据，而数据库里是第二个“ ——–这和我想的是一样的，哈哈 一位名叫 letsgowei 的大佬说：”在做更新操作时不可以更改数据库后直接更新缓存吗？这样最多也就一两次脏数据“ ——–这位大佬的疑惑应该是为什么更新操作只是删除缓存或者把缓存设置为无效，而不是更新缓存呢？还有一位评论有同样的疑惑；这个问题我没有考虑到，觉得他们说的有道理直接更新也可以啊？ 一位名叫 longsen 的大佬做出了解答：”1. 读线程查key未在cache中；2.读线程从db读数据；3.写线程更改数据库；4.写线程看key未在cache中，无法更新cache；5.读线程将旧数据写入cache中。这种场景旧数据可能在cache存在很长时间“ ——–大佬给出的解释是cache中没有key无法更新，I don’t think so！至少在Redis的环境下是不存在的，key不存在直接set，key存在直接覆盖，Redis是有这样的命令的，所以这个回答我不同意，如果将写MySQL后将Redis中的key删除的操作，改成更新Redis的Key操作，这样一来就和我上面提到的场景2是一样的的了，读写都更新Redis增加了产生脏数据的概率，所以是不可取的 一位名叫 泪滴 的大佬说：”大神！你的这个更新顺序是建立在更新数据库，更新缓存都不会发生失败的情况下的，单独考虑并发问题得到的顺序！方案1：先更新数据库，再删除缓存，当出现并发问题概率很小(假设概率为R1)，会造成脏数据。当出现网络等问题导致删除缓存失败(假设概率为R2)，会导致之后的请求一直是脏数据。方案2：先删除缓存，再更新数据库，当出现并发问题概率较大(假设概率为R3)，会导致之后的请求一直是脏数据，当出现网络问题，删除缓存成功，更新数据库失败，只会引发一次cache miss，在业务上基本没啥影响。当然为了弥补，我们一般都会设置缓存的过期时间，来缩短出现脏数据的时间。现在问题的关键就是R1+R2和R3的大小问题了，如果大厂，网络基础设施啥的比较牛，当然R1+R2&lt;R3选择方案1比较合适，对于广大小厂来说还真的可能R1+R2&gt;R3那怎么选择，就比较清楚了。“ ——–说实话，他的评论让我眼前一亮，他的分析具体到了应用场景上，而且确实有这样的情况出现，所以说没有最完美的设计只有最合适的设计，给这位大佬点赞 还有一些则是针对于原文中Write Behind Caching Pattern部分的流程图的疑问，例如缓存未命中为什么回写数据再更新数据，直接更新数据不就好了吗？写数据未命中为什么还有判断dirty的标志？等等，这些问题我也不懂，云里雾里的，不知道为什么这么设计，不就是异步更新MySQL吗，搞这么复杂是为什么，后来我冷静的分析一下，这篇文章是缓存更新的套路，当前部分是缓存异步更新MySQL的介绍，而流程图和异步半毛钱关系都没有，为什么？ ——–因为我太垃圾了，文章中 xxPattern 指的是Linux内核中的缓存更新模式，作者是将这些模式应用到分布式环境下缓存更新中，所以说这部分的流程图是指Linux内核的缓存更新而不是分布式环境下的缓存更新，作者在文章中不止一次提到了基础很重要就体现出来了，而且作者也提到宏观的系统架构设计其实和计算机系统结构中微观的设计是相似的，所以想要设计好一个大型的分布式系统必须对计算机系统结构非常了解 综上所述–基础很重要 以上是我拜读耗子哥的《缓存更新的套路》一文后的一些总结和思考，推荐大家读原文哦！]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次Spring事务不回滚的踩坑记录]]></title>
    <url>%2F2019%2F08%2F16%2F%E4%B8%80%E6%AC%A1Spring%E4%BA%8B%E5%8A%A1%E4%B8%8D%E5%9B%9E%E6%BB%9A%E7%9A%84%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[一次Spring事务不回滚的踩坑记录Spring事务不回滚八成是不知道Spring默认在捕获到unchecked异常才会自动回滚，然而我早已踩过个坑，是一个有经验的人，当我自信满满的加上一行 1/0，并在catch中 throw new RuntimeException，debug之后我懵了，咋不回滚呢？重启Tomcat，浏览器缓存清理之后再试一次，还是不行！！！我就难受了，这和我预想的不一样，检查代码没有发现错误，那咋办呢？开始百度吧，百度的结果千篇一律，都是针对不了解Spring默认捕获unchecked异常的解决办法，这些早已在我的经验里了，有3种方法 1.手动抛出unchecked异常，让Spring去捕获，然后自动回滚数据 2.手动回滚，在发生异常的地方添加代码 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); 3.在注解的地方添加配置rollbackFor = { Exception.class }，让Spring在捕获到特定的异常自动回滚数据 3种方法我都知道，但是我一般只用第一种，因为简单，这次我选择用第二种方法试下，竟然没问题了，我意识到是我的问题了，开始检查代码，我的代码逻辑如下（见笑） 1234567891011121314boolean result = false;try &#123; // ...业务逻辑 System.out.println(1/0); // ...业务逻辑 result = true;&#125; catch (Exception e) &#123; LOGGER.error(e.getMessage(), e); // rollback throw new RuntimeException(e); result = false;&#125; finally &#123; return result;&#125; 还是不知道错在哪里，没有办法开始Debug，惊奇的发现RuntimeException竟然被忽略了，这才发现我finally中有return，被我自己蠢哭了，基础真是太重要了，我还盲目自信的知道Spring的事务如何使用，到头来连try catch finally都没搞清楚，真是太蠢了。接着我修改了代码： 123456789101112boolean result = false;try &#123; // ...业务逻辑 System.out.println(1/0); // ...业务逻辑 result = true;&#125; catch (Exception e) &#123; LOGGER.error(e.getMessage(), e); // rollback throw new RuntimeException(e);&#125; return result; 这下确实是回滚了，但是返回值是true，想得到的时false，这又难受了，再次Debug，很多次F6后我明白了，RuntimeException是被Spring框架里的层层代理catch了————————————————————————————————————————————————————————————–我把我自己给骗了，RuntimeException抛出程序已经终止了，即使再多的catch最后也不会回到result = true那一行，最终得出原因是其他ajax请求的结果返回到了前台给的提示让我误解了 到这里我意识到自己是真的菜，补习一下try catch finally吧找到一篇好文 总结一下 如果finally中有return，try和catch中的return会失效，并且catch中即使抛出unchecked异常也同样会失效（这是今天踩的坑）；如果finally中有异常相当于整个方法有了异常，那么就没有最终的返回值了,catch中有了异常同样的效果，所以catch和finally中不要出现异常 如果finally中没有return，try和catch中走最先到达return逻辑的地方，并且在return前将返回值暂存，即使finally中修改也不会有效果；（也就是说没有异常最先到达try块中的return，返回值是try块的返回值，catch和finally修改也不会生效；如果try块有异常最先到达catch块中的return，返回值是catch块的返回值，前提是catch块中没有异常，有异常整个方法都没有返回值） 综上所述，使用Spring事务避免不出错优先使用方法2和方法3，方法1比较绕并且对有返回值的逻辑不是很友好；finally块中尽量不要return，这样会忽略try和catch中的异常；最后，基础真的很重要]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Optional 使用及源码]]></title>
    <url>%2F2019%2F08%2F15%2FOptional%2F</url>
    <content type="text"><![CDATA[Optional 使用及源码分析A container object which may or may not contain a non-null value. 可能包含空值的容器对象。 怎么理解呢？就把它当成是和Collection一样的容器，Collection是通过不同的数据结构和API来操作容器中的元素；Optional则是提供API来判断容器中的元素是否为空，在此基础上还能根据是否为空的不同结果给出自定义的处理逻辑。这么说还是很抽象，直接上源码就会好理解一些。 成员变量1234// 空的Optional对象private static final Optional&lt;?&gt; EMPTY = new Optional&lt;&gt;();// 容器中元素的值private final T value; 这个value是容器中元素的值，怎么理解呢，使用Optional是要通过它的API进行判空来达到避免NPE的现象，上面说到将Optional当成是一个容器，这个容器中的元素则是需要判空的对象，也就是说容器中的元素就是你传入的参数，这个value就是传参的值 构造方法无参构造，只是将value置为null 123private Optional() &#123; this.value = null;&#125; 有参构造 123private Optional(T value) &#123; this.value = Objects.requireNonNull(value);&#125; 其中返回Objects中的requireNonNull的方法，再看这个方法 12345public static &lt;T&gt; T requireNonNull(T obj) &#123; if (obj == null) throw new NullPointerException(); return obj;&#125; 很简单如果obj为空抛出异常，不为空返回本身，所以有参构造的效果就是确认value不为空并给value赋值，如果是空就抛异常 而且这两个构造函数是私有的，也就是说我们不能new出来 主要方法 empty()–返回空的Optional对象 1234public static &lt;T&gt; Optional&lt;T&gt; empty() &#123; Optional var0 = EMPTY; return var0;&#125; of(T var1)–调用了有参构造，即有值返回带有该值得Optional对象，为空则会抛异常 123public static &lt;T&gt; Optional&lt;T&gt; of(T var0) &#123; return new Optional(var0);&#125; ofNullable(T var0)–元素为null返回空的Option对象，不是null返回本身 123public static &lt;T&gt; Optional&lt;T&gt; ofNullable(T var0) &#123; return var0 == null ? empty() : of(var0);&#125; get()–从名字就可以看出是获取元素的值，也就是返回value，如果是null的话会抛异常 1234567public T get() &#123; if (this.value == null) &#123; throw new NoSuchElementException(&quot;No value present&quot;); &#125; else &#123; return this.value; &#125;&#125; isPresent()–返回value是否为null 123public boolean isPresent() &#123; return this.value != null;&#125; ifPresent(Consumer&lt;? super T&gt; var)–如果元素不是空的话执行var1中的逻辑，Consumer之前有文章写过，是接收一个参数执行一个没有返回值得逻辑 12345public void ifPresent(Consumer&lt;? super T&gt; var1) &#123; if (this.value != null) &#123; var1.accept(this.value); &#125;&#125; filter(Predicate&lt;? super T&gt; var1)–首先确保predicate对象和value不是null，然后用predicate对象对value进行筛选，满足条件返回本身，不满足条件返回空的对象（看源码是这个意思，具体怎什么情况用还想不到~） 12345678public Optional&lt;T&gt; filter(Predicate&lt;? super T&gt; var1) &#123; Objects.requireNonNull(var1); if (!this.isPresent()) &#123; return this; &#125; else &#123; return var1.test(this.value) ? this : empty(); &#125;&#125; map(Function&lt;? super T, ? extends U&gt; var1)–同样确保var1不是null，之后value为空值返回空的Optional对象，value有值执行var1中的逻辑 1234public &lt;U&gt; Optional&lt;U&gt; map(Function&lt;? super T, ? extends U&gt; var1) &#123; Objects.requireNonNull(var1); return !this.isPresent() ? empty() : ofNullable(var1.apply(this.value));&#125; flatMap(Function&lt;? super T, Optional&lt; U &gt;&gt; var1)–与map方法相同,不同的是入参，根据不同的参数结构使用不同的方法 1234public &lt;U&gt; Optional&lt;U&gt; flatMap(Function&lt;? super T, Optional&lt;U&gt;&gt; var1) &#123; Objects.requireNonNull(var1); return !this.isPresent() ? empty() : (Optional)Objects.requireNonNull(var1.apply(this.value));&#125; T orElse(T var1)–获取value的值，不为空返回本身，为空返回入参var1 123public T orElse(T var1) &#123; return this.value != null ? this.value : var1;&#125; T orElseGet(Supplier&lt;? extends T&gt; var1)–与orElse的逻辑一样，不同的是value为空返回的是supplier对象的逻辑 123public T orElseGet(Supplier&lt;? extends T&gt; var1) &#123; return this.value != null ? this.value : var1.get();&#125; T orElseThrow(Supplier&lt;? extends X&gt; var1)–同样的逻辑，不同的是value为null会抛异常 1234567public &lt;X extends Throwable&gt; T orElseThrow(Supplier&lt;? extends X&gt; var1) throws X &#123; if (this.value != null) &#123; return this.value; &#125; else &#123; throw (Throwable)var1.get(); &#125;&#125; 总结 of和ofNullable 都是取值，如果元素是null的话of会报空指针–不用，ofNullable将null转为空的对象没有空指针； get方法同样是取值，value是null也会抛异常–不用 最后，取值用ofNullable就完事了 isPresent和ifPresent isPresent返回元素是否为null，有返回值 ifPresent元素不为空执行一段逻辑，无返回值 最后，只判断用isPresent有逻辑用ifPresent filter、map和flatMap 都是将不是null的元素执行传入的逻辑，根据不同的需求选择方法 orElse、orElseGet和orElseThrow 都是将null的元素做转换，orElse返回传入的值，orElseGet返回传入的逻辑，这两个方法看需求没有逻辑有orElse有逻辑用orElseGet；orElseThrow元素为null抛异常–不用 栗子刚刚学习还不知道怎么使用，看到[一篇文章]（https://www.cnblogs.com/rjzheng/p/9163246.html） 给的栗子不错，很有借鉴意义，但是我对这篇文章中的orElse和orElseGet的栗子有不同意见。 栗子1 使用前 1234567891011public String getCity(User user) throws Exception&#123; if(user!=null)&#123; if(user.getAddress()!=null)&#123; Address address = user.getAddress(); if(address.getCity()!=null)&#123; return address.getCity(); &#125; &#125; &#125; throw new Excpetion(&quot;取值错误&quot;); &#125; 使用后 123456public String getCity(User user) throws Exception&#123; return Optional.ofNullable(user) .map(u-&gt; u.getAddress()) .map(a-&gt;a.getCity()) .orElseThrow(()-&gt;new Exception(&quot;取指错误&quot;));&#125; 栗子2 使用前 123if(user!=null)&#123; dosomething(user);&#125; 使用后 1234Optional.ofNullable(user) .ifPresent(u-&gt;&#123; dosomething(u); &#125;); 栗子3 使用前 123456789101112public User getUser(User user) throws Exception&#123; if(user!=null)&#123; String name = user.getName(); if(&quot;zhangsan&quot;.equals(name))&#123; return user; &#125; &#125;else&#123; user = new User(); user.setName(&quot;zhangsan&quot;); return user; &#125;&#125; 使用后 123456789public User getUser(User user) &#123; return Optional.ofNullable(user) .filter(u-&gt;&quot;zhangsan&quot;.equals(u.getName())) .orElseGet(()-&gt; &#123; User user1 = new User(); user1.setName(&quot;zhangsan&quot;); return user1; &#125;);&#125;]]></content>
      <categories>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>Java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac 设置]]></title>
    <url>%2F2019%2F08%2F13%2Fmac-%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[外接键盘的调整键盘设置中将control和command互换就可以达到和Windows下的复制粘贴时一样的，切换程序则由alt+tab变成了ctrl+tab需要适应一下，不过复制，粘贴，撤销，保存这些不用再去适应新的快捷键了 idea中的调整preferences中找到keymap选择Eclipse(macOS)，这样加上第一步的配置复制粘贴这些快捷键与Windows相同，不用再去适应新的快捷键 自动补全变量keymap中搜索variable 默认为：option+command+L 通过以上设置后为：Ctrl+alt+L 另一种使用方法不做任何配置，idea中使用eclipse风格的快捷键，idea中的使用不影响，但是在idea以外就得适应mac中的快捷键，感觉还是这个方法更容易接受一些]]></content>
  </entry>
  <entry>
    <title><![CDATA[Lambda表达式]]></title>
    <url>%2F2019%2F08%2F05%2FLambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[语法包含3个部分：参数 -&gt; 表达式/代码块 (params) -&gt; expression (params) -&gt; statement (params) -&gt; { statements } 与内部类相同，lambda表达式不可以修改外部变量，这点与匿名内部类相同，不同的是lambda表达式不用将变量显示的声名为final，如果是在自己的作用域中定义局部变量可以进行修改，最终保证线程安全 （踩坑）lambda表达式中的this并不是代表当前使用lambda表达式的对象，而是外部类的对象 作用可代替匿名内部类 可以代替只包含一个抽象方法的接口，也叫做函数式接口，例如；Comparator、Runnable Java8内置了四大函数式接口分别为：Consumer，Supplier，Function，Predicate jdk8中提供@FunctionalInterface 注解来检查接口是否符号函数式接口的标准 可代替迭代操作 list.forEach(n -&gt; {}); 通过Stream操作集合 list.stream().filter()…collect(); 对数据处理 与Spark相似java8可以将集合转化为流（Stream），在对流进行map和reduce操作，与Spark相同这些方法也是惰性求值的 Java8的函数式接口消费型接口 Consumer 抽象方法-void accept(T t); 参数类型-T 返回类型-void 这个还没有用过，因为返回值为空并且传递一个参数，我感觉和集合的遍历差不多 list.forEach(n -&gt; sout(n)); 通过定义多个Consumer对象相当于定义多个逻辑块，最终consumer1.addThen(consumer2) 连接，也就是说consumer1逻辑完成后执行consumer2（为什么不写在一个逻辑里呢？我猜可能需要解耦吧） 供给型接口 Supplier 抽象方法-T get(); 参数类型-无参数 返回类型-T 这个感觉很简单，没有参数但要返回一个值，可能new一个对象的时候会用到吧，声名Supplier对象后直接调用get执行定义的逻辑（箭头后面的逻辑）返回一个值 函数型接口 Function&lt;T,R&gt; 抽象方法-R apply(T t) 参数类型-T 返回类型-R 同样是创建Function对象定义一个方法逻辑，接口中有Consumer接口同样的实现方法andThen，用法也相同，不同的是Function定义中有返回值，fun1.addThen(fun2)是将fun1执行的返回值传入fun2中再执行fun2中的逻辑，除此之外该接口还有一个实现方法compose，用法和andThen相反，fun1.compose(fun2) 是先执行fun2中的逻辑将返回值作为参数传入fun1中再执行fun1中的逻辑 断言型接口 Predicate 抽象方法-boolean test(T t) 参数类型-T 返回类型-boolean 定义的Predicate对象相当于筛选条件的对象，最终通过stream中的filter进行过滤，多个条件可以用and和or来进行组合相当于运算符 &amp;&amp; 和 || 多用做集合筛选 eg: 1234// 筛选大于18岁的女性用户Predicate&lt;User&gt; matchAge = u -&gt; u.age &gt; 18;Predicate&lt;User&gt; matchSex = u -&gt; u.sex.equals(&quot;f&quot;);resultList = userList.stream().filter(matchAge.and(matchSex)).collect(Collectors.toList()); 我的理解是在定义Predicate的对象时，-&gt; 前传入参数， -&gt; 后定义test的方法体，最终补充抽象方法test，通过stream的filter筛选相当于将集合中的每个元素都调用一次test方法，将返回值为true的筛选出来。]]></content>
      <tags>
        <tag>Java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StringUtils]]></title>
    <url>%2F2019%2F07%2F26%2FStringUtils%2F</url>
    <content type="text"><![CDATA[StringUtils 工作中很多操作字符串的操作，使用到了工具类这里总结下，org.apache.commons.lang3包下的 split(String str, String separatorChars)–&gt;切分字符串123public static String[] split(String str, String separatorChars) &#123; return splitWorker(str, separatorChars, -1, false);&#125; 参数： int max -&gt;the maximum number of elements to include in the array. A zero or negative value implies no limit.这个参数代表返回的字符串的最大长度，0或者-1代表不限制长度 boolean preserveAllTokens -&gt; if {@code true}, adjacent separators are treated as empty token separators; if {@code false}, adjacent separators are treated as one separator. 这个参数是连续分隔符规则的标志，如果为true连续的分隔符都会匹配，最终得到的字符串数组会有空的值，jdk中的split就是这个规则；如果为false，连续的分隔符只会匹配一次，最终得到的数组不会有空值。eg(“1,2,3,,4,5”切分后，true得到[1,2,3,,4,5]而false得到[1,2,3,4,5])， 这也是与jdk中的split方法的区别，如果需要使用与jdk相同的规则，工具类中的splitPreserveAllTokens方法可以实现，该方法会调用splitWorker方法且最后的参数为true 所以split方法默认参数为-1和false表示数组长度不收限制，及使用第二个规则进行切割，确保得到的字符串数组没有空值原理： 先将字符串与分隔符做匹配 匹配到之后将分隔符之前的子串分割add到一个list集合中 最后使用list.toArray返回最终的数组 join 待续]]></content>
      <tags>
        <tag>utils</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Arrays]]></title>
    <url>%2F2019%2F07%2F26%2FArrays%2F</url>
    <content type="text"><![CDATA[ArrayscopyOf123456789public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) &#123; @SuppressWarnings(&quot;unchecked&quot;) T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy;&#125; 先看要被拷贝到的数组长度是不是够用，够用的话直接调用System.arraycopy方法；不够用创建一个新的与源数组同样长度的数组进行拷贝如果数组中是引用类型，Arrays.copy拷贝的是引用，不会新创建对象，如果要对拷贝的数组做修改操作源数组同样会受到影响，而字符串数组由于字符串常量池的存在，当修改字符串的时候会新创建一个字符串并将新的引用付给数组，所以源数组对应的字符串并不会发生变化 System.arraycopy123public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); 这是一个本地方法，就看一下参数吧 src—-the source array. srcPos—-starting position in the source array. dest—-the destination array. destPos—-starting position in the destination data. length—-the number of array elements to be copied. asList将字符串转成ArrayList集合 123public static &lt;T&gt; List&lt;T&gt; asList(T... a) &#123; return new ArrayList&lt;&gt;(a);&#125; 这里的ArrayList是Arrays中的一个内部类，继承了AbstractList方法，内部值实现了部分方法，简单点说这个集合是只读的，不能进行修改和删除操作，因为没有重写相关的方法。 copyOfRange按照范围拷贝数组 [from,to) 左开右闭123public static &lt;T&gt; T[] copyOfRange(T[] original, int from, int to) &#123; return copyOfRange(original, from, to, (Class&lt;? extends T[]&gt;) original.getClass());&#125; sort集合工具类 Collections.sort 其实就是调用 Arrays.sort 方法对集合进行排序的，该方法先调用 toArray 方法将集合转成object数组，然后再调用 Arrays.sort 方法对数组进行排序，最后再将排序号的数组通过迭代器set到新的集合中去。 123456public static void sort(Object[] a) &#123; if (LegacyMergeSort.userRequested) legacyMergeSort(a); else ComparableTimSort.sort(a, 0, a.length, null, 0, 0);&#125; 可以看到sort方法是通过userRequested的标志来选中排序的方式，从jdk7以后默认为false，使用TimSort的方式排序，（通过System.setProperty(“java.util.Arrays.useLegacyMergeSort”, “true”)修改） userRequested为true使用LegacyMergeSort的方式进行排序，当数组长度小于7时使用插入排序，当数组长度大于7时使用归并排序，归并到长度小于7的长度再次使用插入排序 userRequested为false采用TimSort的方式排序 TimSort 1.数组长度小于32时，首先在数组中从开头开始寻找升序的子数组，没有的话找降序的子数组再反转，然后将数组中的剩余元素使用二分查找的方式插入到子数组中 2.数组长度大于32时，将数组切分若干个长度在[16,32)的区块（jdk里叫run，我理解为区块） 3.每个区块再使用第一步的方式进行排序排序后将每个区块进行合并，合并的过程有两点优化 a.合并区块的过程中通过限制条件来完成将连续的三个区块中较小的两个优先合并降低复杂度 b.两个区块合并时，先将区块1的头元素和尾元素插入到区块2中，相当于缩小了插入区块2的范围降低复杂度 12345678910111213141516171819202122232425262728293031323334353637383940414243444546static void sort(Object[] a, int lo, int hi, Object[] work, int workBase, int workLen) &#123; assert a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; int nRemaining = hi - lo; if (nRemaining &lt; 2) return; // Arrays of size 0 and 1 are always sorted // If array is small, do a &quot;mini-TimSort&quot; with no merges if (nRemaining &lt; MIN_MERGE) &#123; int initRunLen = countRunAndMakeAscending(a, lo, hi); binarySort(a, lo, hi, lo + initRunLen); return; &#125; /** * March over the array once, left to right, finding natural runs, * extending short natural runs to minRun elements, and merging runs * to maintain stack invariant. */ ComparableTimSort ts = new ComparableTimSort(a, work, workBase, workLen); int minRun = minRunLength(nRemaining); do &#123; // Identify next run int runLen = countRunAndMakeAscending(a, lo, hi); // If run is short, extend to min(minRun, nRemaining) if (runLen &lt; minRun) &#123; int force = nRemaining &lt;= minRun ? nRemaining : minRun; binarySort(a, lo, lo + force, lo + runLen); runLen = force; &#125; // Push run onto pending-run stack, and maybe merge ts.pushRun(lo, runLen); ts.mergeCollapse(); // Advance to find next run lo += runLen; nRemaining -= runLen; &#125; while (nRemaining != 0); // Merge all remaining runs to complete sort assert lo == hi; ts.mergeForceCollapse(); assert ts.stackSize == 1;&#125;]]></content>
      <tags>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList]]></title>
    <url>%2F2019%2F07%2F26%2FArrayList%2F</url>
    <content type="text"><![CDATA[ArrayListtoArray(T[] a)123456789public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) // Make a new array of a&apos;s runtime type, but my contents: return (T[]) Arrays.copyOf(elementData, size, a.getClass()); System.arraycopy(elementData, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a;&#125; 使用了Arrays.copyOf方法]]></content>
      <tags>
        <tag>collections</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis入门（续）-CRUD]]></title>
    <url>%2F2019%2F07%2F12%2FMyBatis%E5%85%A5%E9%97%A8%E7%BB%AD-CRUD%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[根据用户名查询 123&lt;select id=&quot;findUserByName&quot; parameterType=&quot;java.lang.String&quot; resultType=&quot;com.example.mybatisdemo.bean.User&quot;&gt; SELECT * FROM users WHERE name = #&#123;VALUE&#125; &lt;/select&gt; 1sqlSession.selectOne(&quot;test.findUserByName&quot;, &quot;yywang&quot;) 模糊查询，返回多个值 123&lt;select id=&quot;findUserLikeName&quot; parameterType=&quot;java.lang.String&quot; resultType=&quot;com.example.mybatisdemo.bean.User&quot;&gt; SELECT * FROM users WHERE name like #&#123;VALUE&#125; &lt;/select&gt; 1sqlSession.selectList(&quot;test.findUserLikeName&quot;, &quot;%yy%&quot;); 查询的resutlType分三种情况 基本类型：resultType=”基本类型” List类型：resultType=”List集合中的元素类型” Map类型： 单条记录 resultType=”java.util.Map” 多条记录 resultType=”Map中value的类型” 添加数据 123456&lt;insert id=&quot;insertUser&quot; parameterType=&quot;com.example.mybatisdemo.bean.User&quot;&gt; &lt;selectKey keyProperty=&quot;id&quot; order=&quot;AFTER&quot; resultType=&quot;int&quot;&gt; SELECT LAST_INSERT_ID() &lt;/selectKey&gt; INSERT into users(uname,sex,age,udesc) values (#&#123;uname&#125;,#&#123;sex&#125;,#&#123;age&#125;,#&#123;udesc&#125;) &lt;/insert&gt; 12User user = new User(&quot;bangni&quot;,&quot;female&quot;,22,&quot;tc&quot;);sqlSession.commit(); // 必加 tips selectKey 用来配置返回主键 keyProperty 表中主键的名称 order 表示SELECT LAST_INSERT_ID()在insert语句发生的顺序，after意为insert执行之后返回，用于自增主键，UUID的方式可以配置为before resultType 返回值类型 注1：sql语句中有多个参数，占位符#{}也需要指定不同的表示方式，如上#{uname},#{sex}等 注2：sql没问题运行报错，因为之前的数据表设计问题，name和desc是关键字，这里开始做了修改 注3：修改之后运行通过，数据库查不到记录，想到之前测试Junit回自动回滚，于是添加@Rollback注解导入依赖后还是无果，最终加上session.commit()解决，由于MyBatis接管了JDBC的事务管理器，JDBC回自动提交而MyBatis不会，这里需要自行手动提交，修改删除同样 删除 123&lt;delete id=&quot;delUserById&quot; parameterType=&quot;int&quot;&gt; delete from users where id = #&#123;id&#125; &lt;/delete&gt; 12sqlSession.delete(&quot;test.delUserById&quot;,3); sqlSession.commit(); 更新 123&lt;update id=&quot;updateUserById&quot; parameterType=&quot;int&quot;&gt; update users set age = 0 where id = #&#123;id&#125; &lt;/update&gt; 12sqlSession.update(&quot;test.updateUserById&quot;,8); sqlSession.commit(); 查看最后执行的SQL只需在配置文件中添加配置即可打印查询语句12345&lt;configuration&gt; &lt;settings&gt; &lt;setting name=&quot;logImpl&quot; value=&quot;STDOUT_LOGGING&quot; /&gt; &lt;/settings&gt;&lt;/configuration&gt;]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA配置Junit测试]]></title>
    <url>%2F2019%2F07%2F12%2FIDEA%E9%85%8D%E7%BD%AEJunit%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[看了很多博客后感觉还是比较乱，这篇还不错马一下https://blog.csdn.net/hanchao5272/article/details/79197989 1.安装插件File-&gt;setting-&gt;Plugins-&gt;搜索并安装Junit Generator 2.0-&gt;重启IDEA 2.配置插件File-&gt;setting-&gt; OtherSettings-&gt;Junit Generator-&gt;properties 修改Output Path[输出路径]为${SOURCEPATH}/../../test/java/${PACKAGE}/${FILENAME} 修改 Default Template[默认模板]为JUnit4 选中JUnit4页签，将package test.$entry.packageName; 修改成package $entry.packageName; 3.配置测试的目录File-&gt;Project Structure-&gt;Modules中将测试目录设置为Test Source Floder 4.生成测试类 在要测试的类中用快捷键 alt+insert -&gt; Junit Test -&gt; Junit4 5.测试 鼠标右键菜单 将鼠标光标放在方法相关代码中，右键弹出菜单中会显示运行此测试方法的菜单，点击就会运行方法单独测试。将鼠标光标放在方法之外的代码中，右键弹出菜单中会显示运行此类的所有测试方法的菜单，点击就会运行所有测试方法。 快捷键 将鼠标光标放在方法相关代码中，通过快捷键Ctrl+Shift+F10，运行当前测试方法。 将鼠标光标放在方法之外的代码中，通过快捷键Ctrl+Shift+F10，运行当前类的所有测试方法。 快捷按钮 点击方法左侧的Run Test按钮，运行当前测试方法。 点击类左侧的Run Test按钮，运行当前类的所有测试方法。 6.测试结果 1.方法测试成功 2.方法测试失败 3.测试用时（毫秒） 4.期望值 5.实际值 6.异常信息 7.异常 原因：4.11以上版本不在包含hamcrest 解决：改用4.10 ^_^]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis入门]]></title>
    <url>%2F2019%2F07%2F12%2FMyBatis%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[MyBatis环境首先准备数据库表 对应的实体类为 class User &#123;123456 private int id; private String name; private String sex; private int age; private String desc;&#125; 数据库配置文件 SqlMapConfig.xml 配置数据库环境相关 1234567891011&lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test?characterEncoding=utf-8&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; sql映射文件 user.xml 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;test&quot;&gt; &lt;select id=&quot;findUserById&quot; parameterType=&quot;int&quot; resultType=&quot;com.example.mybatisdemo.bean.User&quot;&gt; SELECT * FROM user WHERE id =#&#123;VALUE&#125; &lt;/select&gt;&lt;/mapper&gt; 将sql映射添加到SqlMapConfig.xml中 最终的配置文件为 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test?characterEncoding=utf-8&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=&quot;mapper/user.xml&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 测试 123456789String resource = &quot;SqlMapConfig.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(inputStream); SqlSession sqlSession = factory.openSession(); // 参数1 sql映射中的 namespace + &quot;.&quot; + sqlId // 参数2为sql的参数 User user = sqlSession.selectOne(&quot;test.findUserById&quot;, 1); System.out.println(user.toString()); sqlSession.close(); 理解基于sql语句的轻量级ORM框架，将sql语句写入配置文件映射中，进一步解耦，但是多了一步操作感觉比hibernate繁琐一些，但是比hibernate要快，有舍有得吧（为什么快还不知道，后续再看吧╮(╯▽╰)╭ ）]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC入门]]></title>
    <url>%2F2019%2F07%2F11%2FSpringMVC%2F</url>
    <content type="text"><![CDATA[SpringMVC配置注解&amp;配置文件 注解web.xml ①指定Spring配置文件的位置 ②配置Listener，初始化SpringIOC容器 ③配置前端控制器servlet，其中可以自定义配置文件位置，不配置默认寻找xxxx-servlet.xml的配置文件 url-pattern中/和/*区别 /* 匹配所有url 有后缀或者无后缀都会匹配 .jsp .css .js / 只匹配无后缀的url 注：截图为项目中的配置 自己测试时改为 / 项目中拦截所有页面应该会有拦截器或者过滤器做处理，demo中如果配置成截图这样会报错 springmvc-servlet.xml 指定基础包名scan，将指定的包名注入SpringIOC容器（先要添加context的xsd约束） 1xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi中添加“http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot; exclude-filter 指定类与Spring容器分开加载（先这么理解） 配置视图解析器（前缀和后缀） 方法中使用@RequestMapping(value=”search”) 理解为匹配URL中search的字样 方法return “iface/manage”; 从匹配的前后缀中寻找应该返回的视图，例如通过上图的配置找到/iface/manage.vm 在Controller类上添加@Controller，方法上添加@RequestMapping(“xxxx”)，即可完成映射 配置完成访问报错 没有jstl标签库，导入依赖即可 1234&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;/dependency&gt; 配置文件web.xml 和注解方式一样springmvc-servlet.xml ①配置处理器映射器 ②配置处理器适配器 ③配置视图解析器（同注解方式） ④配置映射（相当于注解中的@RequestMapping） 相较于注解方式该配置文件中多了对 处理器映射器、处理器适配器 以及映射的配置 实现方面在controller类中不添加任何注解，实现Controller接口，重写方法即可 demo：https://github.com/panniyuyu/frameworkdemo.git 理解通过使用不同方式对springMVC进行配置，感觉对SpringMVC框架大致的原理有一些认识 SpringMVC使将MVC的模式进一步拆分解耦，整个过程主要包含4个主要的部分依次是 前端控制器（DispatcherServlet）、处理器映射器（HandlerMapping）、处理器适配器（HandlerAdapter）、视图解析器（ViewResolver） 1.用户发起请求，被前端控制器（DispatcherServlet）拦截，并根据请求内容询问处理器映射器（HandlerMapping）改请求应该由哪个Controller处理，处理器映射器将匹配到的Controller信息返回给前端控制器 2.前端控制器知道该请求应该由哪个Controller处理，但不会自己处理，将Controller信息交给处理器适配器（HandlerAdapter）处理，返回ModelAndView对象 3.前端控制器得到ModelAndView对象将其转发给视图解析器，将对象解析成view页面返回 4.前端控制器将view页面相应给浏览器]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[org.apache.tomcat.util.bcel.classfile.ClassFormatException]]></title>
    <url>%2F2019%2F07%2F11%2F%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[org.apache.tomcat.util.bcel.classfile.ClassFormatException 原因：jdk版本不兼容 原环境 jkd8+tomcat7+spring4 解决：tomcat7换tomcat8]]></content>
      <tags>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA使用笔记]]></title>
    <url>%2F2019%2F07%2F11%2FIDEA%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[配置TomcatRun-&gt;Edit Configurations-&gt;Telplates中配置后在该页面左上角添加-&gt;选中Tomcat的Deployment点击部署(选用Artifacts方式) 配置文件取消Unicode编码File-&gt;Setting-&gt;搜索file encoding-&gt;勾选Transparent native-to-ascii conversion 文件目录变红色 解除版本控制即可 file-&gt;setting-&gt;version control-&gt;右上角加号-&gt;添加项目目录即可 新建的maven项目没有web项目的目录结构，也没有web.xml 增加main目录下增加/webapp/WEB-INF目录 File-&gt;Project Structure-&gt;facets-&gt;加号-&gt;选中目录 确认路径depolyment路径为…./webapp/WEB-INF/web.xml 确认路径resource路径为 …./webapp/ 直接创建maven web项目最为简单 createProject-&gt;maven-&gt;勾选Creater from archetype-&gt;选择 maven-archetype-webapp 右键没有new package修改目录性质，在该目录右键-&gt;Mark Directory as-&gt;Source Root 发布方式（参考https://www.cnblogs.com/dpl9963/p/10075456.html） jar：Java ARchrive，仅仅是编译好的Java类的聚合 war：Web application ARchrive，除Java类之外还包含jsp，config等静态资源的聚合 exploded：理解为展开不压缩，jar和war是压缩的目录节后，exploded表示不压缩的文件目录，开发是用该方式较好，文件更改后不用重新启动服务器看到效果 Debug模式 快捷键改为eclipse后，F5，F6不变，eclipse的F8变为F9（程序放行） 修改文件后没有效果必须重启tomcat 热部署 runConfigurations中配置 部署项目到tomcat上，这里的url一定要改成 / 启动tomcat日志输出乱码 淇℃伅（https://www.cnblogs.com/Yin-BoKeYuan/p/10320622.html）打开到tomcat安装目录下的conf/文件夹 修改logging.properties文件，找到 java.util.logging.ConsoleHandler.encoding = utf-8更改为 java.util.logging.ConsoleHandler.encoding = GBK Java应用热启动配置方法1.修改之后手动选择 Run-&gt;Reload Changed Classes 不能设置快捷键 方法2.我选择使用Jrebel插件，安装重启后要填激活码 (这里有人搞好了，拿来用^&amp;^ ) 使用的时候原来是点run或者debug run，现在点旁边两个带jrebel的run和debug run即可 修改代码后 ctrl+F9 快速编译就能查看效果，相当于给方法1加了快捷键 import的类不识别，显示红色这个类是存在的，其他类中引用同样的类就正常，编译无数次还是没解决，缓存问题 解决方法： file -&gt; Invalidate Caches / Restart… -&gt; Invalidate and Restart]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识点复习]]></title>
    <url>%2F2019%2F07%2F09%2F%E7%9F%A5%E8%AF%86%E7%82%B9%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[整理一些在看项目时候遇到的小的知识点，先写个大概，后续再做详细的补充 HTTP和TCPHttp是在Tcp的基础之上的，也就是说Http的请求和相应是在建立Tcp链接之后发生的 幂等性一次或者多次请求同一个资源得到的结果是一样的，多次请求不会影响最终的结果。（增加去重的逻辑则无需满足幂等性） synchronized 关键字 在方法中锁住的是该类的实例对象 在静态方法中锁住的是类对象 代码块中（this）锁住的是该类的实例对象 代码块中（xxx.class）锁住的是类对象 volatile保证线程数据可见 transient 关键字不做序列化和反序列化操作 synchronized在方法中声名为什么还用线程安全的数据结构来存放变量该变量可能在其他地方被调用，如果该变量只在synchronized关键字声名在方法中使用，则无需使用线程安全的数据结构。 TPS （Transaction Per Second）服务器每秒处理的事务个数，一个事务是从向服务器发送请求开始，客户端接收到响应结束 QPS （Query Per Second）服务器每秒处理查询的次数，查询开始到返回结果结束 Git克隆分支命令git clone -b [分支名称] [git地址] Git提交代码到GitHub 创建仓库，在本地clone 本地在.git所在的目录打开git bansh 指定远程仓库 git remote add origin https://github.com/panniyuyu/frameworkdemo.git 会提示 remote origin already exists 执行删除命令后再重新指定远程仓库 git remote rm origin 添加文件git add * 提交改动git commit -m “xxxx” 推到远程仓库git push origin master wait()方法 会暂停当前线程，让出CPU时间，同时让出锁，等待notify()或者notifyAll()唤醒后重新获得锁执行 sleep()方法同样会暂停当前线程，让出CPU时间，与 wait()方法不同的是，sleep()方法不会释放锁，会阻塞当前的线程，且sleep()是Thread类中的方法, wait()是Object的方法 守护线程 Java中优先级低的线程，用来服务于用户线程的，当Java程序退出或者jvm退出时，守护线程自动退出，jvm运行时只需关注用户线程即可。 Jvm中的垃圾收集器可以理解为守护线程，当jvm退出时会自动退出 使用 thread.setDaemon(true)设置，要在start()方法之前 Class的isAssignableFrom方法 Class中的方法，如：a.isAssignableFrom(b) 在a是b的父类或接口，亦或是a、b是同一个类或者接口的情况下返回true，其他情况返回false Class的getFields和getDeclaredFields 都是获取类中的字段，getFields获取类中public的字段，getDeclaredFields获取类中所有声名的字段，不包含父类中的字段 Field的getModifiers 获取字段的修饰符，返回值为int型对应不同的类型 PUBLIC: 1 PRIVATE: 2 PROTECTED: 4 STATIC: 8 FINAL: 16 SYNCHRONIZED: 32 VOLATILE: 64 TRANSIENT: 128 NATIVE: 256 INTERFACE: 512 ABSTRACT: 1024 STRICT: 2048 Field的setAccessible(true) 字段被声名是私有的，在取值前必须设置accessible为true，不然会报错 field的getGenericType和getType 都是获取字段的类型，getGenericType返回的是Type类型，getType返回的是Class类型 还有其他不同，暂时没有理解http://www.51gjie.com/java/793.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[第一篇博客]]></title>
    <url>%2F2019%2F05%2F11%2F%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[时光匆匆，三年的研究生生涯马上就要结束。这是我毕业论文致谢里的第一句话，虽然很老套但是非常应景，在毕业之前的这段时间没有那么多的事情，突然想到做一个自己的博客，把自己的学习和生活记录下来，不用每次遇到问题的时候再去问度娘，而且很多都是重复的问题，虽然自己也在做笔记但很少回头看，打算以前的笔记不再管了，当初为了图省事写的非常简单有些已经想不起来是做什么的了，现在想想非常懊悔，这个坏毛病一定要改。今日在我的博客搭建完成之际，开始将今后所学习的技术记录在此，沉淀下去，和大家做交流，同时，在此也将记录我的生活，有趣的所见所闻什么的，朋友圈发的频繁遭人厌。马上就要入职了，心里知道要回归到工程中了，不然入职后的压力会很大，但是也不知道自己工作内容是什么，浏览了一些博客发现需要学习的东西实在是太多了无从下手，而且就我自己而言没有在工程中应用过的技术即使理解了最后也会忘掉，所以学习的情绪很down，想找一些有趣的东西搞一下，于是本站诞生了。emmm……第一篇博客就到这了，自己小学语文水平只能写到这了，给自己加油！]]></content>
      <categories>
        <category>生活杂谈</category>
      </categories>
      <tags>
        <tag>生活杂谈</tag>
      </tags>
  </entry>
</search>
